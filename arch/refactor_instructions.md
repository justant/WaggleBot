# WaggleBot êµ¬ì¡° ê°œì„  ì‘ì—… ì§€ì‹œì„œ

> **ëŒ€ìƒ**: Claude Code
> **ëª©ì **: Dead code ì œê±°, í¬ë¡¤ëŸ¬ ê³µí†µí™”, ì„¤ì • ë¶„ë¦¬, DB ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´ê³„í™”
> **í•„ë…**: ì‘ì—… ì „ ë°˜ë“œì‹œ `CLAUDE.md` ì „ì²´ë¥¼ ì½ê³  ì½”ë”© ê·œì¹™ì„ ì¤€ìˆ˜í•  ê²ƒ

---

## ì‘ì—… ìˆœì„œ (ë°˜ë“œì‹œ ìˆœì„œëŒ€ë¡œ ì§„í–‰)

1. [Task 1] Dead Code ì‚­ì œ
2. [Task 2] BaseCrawler ê³µí†µ í—¬í¼ í†µí•©
3. [Task 3] í¬ë¡¤ëŸ¬ ì„¹ì…˜ ì„¤ì • ë¶„ë¦¬
4. [Task 4] DB ë§ˆì´ê·¸ë ˆì´ì…˜ ëŸ¬ë„ˆ í†µí•©
5. [Task 5] ê²€ì¦

---

## Task 1: Dead Code ì‚­ì œ

### 1-1. ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ì‚­ì œ

ì•„ë˜ íŒŒì¼ë“¤ì€ í”„ë¡œì íŠ¸ì—ì„œ **í•œ ë²ˆë„ ì‹¤í–‰ë˜ì§€ ì•Šê±°ë‚˜**, êµ¬í˜„ì´ ì—†ëŠ” Dead Codeì´ë‹¤. ì‚­ì œí•˜ë¼.

```bash
# YAML ê¸°ë°˜ ë²”ìš© í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ (ì‹¤ì œ í¬ë¡¤ëŸ¬ 4ê°œ ëª¨ë‘ í•˜ë“œì½”ë”© í´ë˜ìŠ¤ë¼ ì‚¬ìš© ì•ˆ ë¨)
rm crawlers/configurable_crawler.py
rm crawlers/site_loader.py
rm config/sites.yaml

# êµ¬í˜„ ì—†ëŠ” ë¹ˆ í¬ë¡¤ëŸ¬ (enabled=False, ë©”ì„œë“œê°€ ì „ë¶€ return [] / log.warning)
rm crawlers/nate_tok.py
```

### 1-2. ì‚­ì œ í›„ ì°¸ì¡° ì •ë¦¬

ì‚­ì œí•œ íŒŒì¼ì„ importí•˜ê±°ë‚˜ ì°¸ì¡°í•˜ëŠ” ê³³ì´ ìˆìœ¼ë©´ ì œê±°í•˜ë¼.

**í™•ì¸í•  íŒŒì¼ë“¤:**

- `crawlers/__init__.py` â€” `nate_tok`, `configurable_crawler`, `site_loader` importê°€ ìˆìœ¼ë©´ ì œê±°
- `crawlers/plugin_manager.py` â€” `auto_discover()` ë©”ì„œë“œ ë‚´ë¶€ì—ì„œ `configurable_crawler` ë˜ëŠ” `site_loader`ë¥¼ ì°¸ì¡°í•˜ëŠ” ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì œê±°
- `main.py` (í¬ë¡¤ëŸ¬ ì§„ì…ì ) â€” `site_loader.load_site_configs()` í˜¸ì¶œì´ ìˆìœ¼ë©´ ì œê±°
- `config/settings.py` â€” `NATE_TOK_SECTIONS` ê°™ì€ ë³€ìˆ˜ê°€ ìˆìœ¼ë©´ ì œê±°

**í™•ì¸ ë°©ë²•:**
```bash
grep -r "configurable_crawler\|site_loader\|nate_tok\|sites\.yaml\|load_site_configs\|ConfigurableCrawler\|SiteConfigLoader" --include="*.py" .
```

ê²°ê³¼ì— ë‚˜ì˜¤ëŠ” ëª¨ë“  ì°¸ì¡°ë¥¼ ì œê±°í•˜ë¼. ë‹¨, ì´ ì§€ì‹œì„œ íŒŒì¼ ìì²´ë‚˜ `ADDING_CRAWLER.md`ëŠ” ì œì™¸.

### 1-3. ADDING_CRAWLER.md ì—…ë°ì´íŠ¸

`crawlers/ADDING_CRAWLER.md` íŒŒì¼ì— YAML ê¸°ë°˜ í¬ë¡¤ëŸ¬ ê´€ë ¨ ì–¸ê¸‰ì´ ìˆìœ¼ë©´ ì œê±°í•˜ë¼.
"sites.yaml", "ConfigurableCrawler" ë“±ì˜ ë¬¸êµ¬ë¥¼ ê²€ìƒ‰í•´ì„œ í•´ë‹¹ ë¶€ë¶„ì„ ì‚­ì œ.

### 1-4. plugin_manager.pyì˜ nate_tok ë“±ë¡ ì œê±° í™•ì¸

`nate_tok.py` ì‚­ì œ í›„ `CrawlerRegistry`ì— `nate_tok`ì´ ë‚¨ì•„ìˆì§€ ì•ŠëŠ”ì§€ í™•ì¸.
`@CrawlerRegistry.register` ë°ì½”ë ˆì´í„°ê°€ íŒŒì¼ import ì‹œì ì— ì‹¤í–‰ë˜ë¯€ë¡œ, íŒŒì¼ ì‚­ì œë§Œìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤.
ë‹¨, `auto_discover()`ê°€ ì‚­ì œëœ íŒŒì¼ì„ import ì‹œë„í•˜ë©´ ì—ëŸ¬ê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸í•˜ë¼.

---

## Task 2: BaseCrawler ê³µí†µ í—¬í¼ í†µí•©

### 2-1. í˜„ì¬ ë¬¸ì œ

4ê°œ í¬ë¡¤ëŸ¬(`nate_pann.py`, `bobaedream.py`, `dcinside.py`, `fmkorea.py`)ê°€ ì•„ë˜ ì½”ë“œë¥¼ **ê°ê° ë³µì‚¬**í•´ì„œ ì‚¬ìš© ì¤‘ì´ë‹¤:

| ì¤‘ë³µ ì½”ë“œ | ì„¤ëª… |
|---|---|
| `self._session = requests.Session()` + `headers.update()` | ìƒì„±ìì—ì„œ ì„¸ì…˜ ì´ˆê¸°í™” |
| `_rotate_ua()` | User-Agent ëœë¤ ë¡œí…Œì´ì…˜ |
| `_parse_int(s: str) -> int` | ë¬¸ìì—´ì—ì„œ ìˆ«ì ì¶”ì¶œ |
| `_parse_stat(text, pattern) -> int` | ì •ê·œì‹ìœ¼ë¡œ í†µê³„ ìˆ«ì ì¶”ì¶œ |

### 2-2. BaseCrawler ìˆ˜ì • (`crawlers/base.py`)

`BaseCrawler` í´ë˜ìŠ¤ì— ì•„ë˜ ê³µí†µ ë©”ì„œë“œì™€ `__init__`ì„ ì¶”ê°€í•˜ë¼.
**ê¸°ì¡´ `run()`, `_upsert()`, `_sync_comments()`, `calculate_engagement_score()`ëŠ” ì ˆëŒ€ ìˆ˜ì •í•˜ì§€ ë§ ê²ƒ.**

```python
import random
import re
import requests

from config.settings import REQUEST_HEADERS, REQUEST_TIMEOUT, USER_AGENTS

class BaseCrawler(ABC):
    site_code: str = ""

    def __init__(self) -> None:
        self._session = requests.Session()
        self._session.headers.update(REQUEST_HEADERS)

    # --- ê³µí†µ HTTP ---

    def _rotate_ua(self) -> None:
        """User-Agentë¥¼ ëœë¤ìœ¼ë¡œ êµì²´í•œë‹¤."""
        self._session.headers["User-Agent"] = random.choice(USER_AGENTS)

    def _get(self, url: str, **kwargs) -> requests.Response:
        """UA ë¡œí…Œì´ì…˜ + íƒ€ì„ì•„ì›ƒì´ ì ìš©ëœ GET ìš”ì²­."""
        self._rotate_ua()
        kwargs.setdefault("timeout", REQUEST_TIMEOUT)
        resp = self._session.get(url, **kwargs)
        resp.raise_for_status()
        return resp

    def _post(self, url: str, **kwargs) -> requests.Response:
        """UA ë¡œí…Œì´ì…˜ + íƒ€ì„ì•„ì›ƒì´ ì ìš©ëœ POST ìš”ì²­."""
        self._rotate_ua()
        kwargs.setdefault("timeout", REQUEST_TIMEOUT)
        resp = self._session.post(url, **kwargs)
        resp.raise_for_status()
        return resp

    # --- ê³µí†µ íŒŒì‹± ---

    @staticmethod
    def _parse_int(s: str) -> int:
        """ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ int ë°˜í™˜. ìˆ«ì ì—†ìœ¼ë©´ 0."""
        digits = re.sub(r"[^\d]", "", s)
        return int(digits) if digits else 0

    @staticmethod
    def _parse_stat(text: str, pattern: str) -> int:
        """ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ í†µê³„ ìˆ«ìë¥¼ ì¶”ì¶œí•œë‹¤."""
        m = re.search(pattern, text)
        if not m:
            return 0
        return int(m.group(1).replace(",", ""))

    @staticmethod
    def _text(el) -> str:
        """BeautifulSoup ìš”ì†Œì˜ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•œë‹¤."""
        return el.get_text(strip=True) if el else ""

    # ... ê¸°ì¡´ abstractmethod, run(), _upsert(), _sync_comments() ìœ ì§€ ...
```

**ì¤‘ìš”**: `import random`, `import re`, `import requests`ì™€ settings importë¥¼ `base.py` ìƒë‹¨ì— ì¶”ê°€í•˜ë¼.
ê¸°ì¡´ `import hashlib`, `import logging`, `from abc import ABC, abstractmethod` ë“±ì€ ìœ ì§€.

### 2-3. 4ê°œ í¬ë¡¤ëŸ¬ì—ì„œ ì¤‘ë³µ ì½”ë“œ ì œê±°

ê° í¬ë¡¤ëŸ¬ íŒŒì¼ì—ì„œ ì•„ë˜ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë¼:

#### (A) ìƒì„±ì ë³€ê²½

**Before** (4ê°œ íŒŒì¼ ëª¨ë‘ ë™ì¼ íŒ¨í„´):
```python
def __init__(self):
    self._session = requests.Session()
    self._session.headers.update(REQUEST_HEADERS)
```

**After**:
```python
def __init__(self) -> None:
    super().__init__()
    # ì‚¬ì´íŠ¸ ê³ ìœ  í—¤ë”ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
    # ì˜ˆ: self._session.headers["Referer"] = "https://www.fmkorea.com/"
```

- `nate_pann.py`: ì¶”ê°€ í—¤ë” ì—†ìŒ â†’ `__init__` ì‚­ì œ ê°€ëŠ¥ (BaseCrawler ê²ƒ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
- `bobaedream.py`: ì¶”ê°€ í—¤ë” ì—†ìŒ â†’ `__init__` ì‚­ì œ ê°€ëŠ¥
- `dcinside.py`: `Referer` í—¤ë” ìˆìŒ â†’ `super().__init__()` í˜¸ì¶œ í›„ `self._session.headers["Referer"] = ...` ì¶”ê°€
- `fmkorea.py`: `Referer` í—¤ë” ìˆìŒ â†’ ìœ„ì™€ ë™ì¼

#### (B) ì¤‘ë³µ ë©”ì„œë“œ ì‚­ì œ

4ê°œ íŒŒì¼ ëª¨ë‘ì—ì„œ ì•„ë˜ ë©”ì„œë“œë¥¼ **ì‚­ì œ**í•˜ë¼ (BaseCrawlerì—ì„œ ìƒì†ë°›ìœ¼ë¯€ë¡œ):

- `_rotate_ua()` â€” 4ê°œ íŒŒì¼ ëª¨ë‘ ì‚­ì œ
- `_parse_int()` â€” 4ê°œ íŒŒì¼ ëª¨ë‘ ì‚­ì œ (ë‹¨, `nate_pann.py`ë„ `@staticmethod`ë¡œ ë™ì¼ ë¡œì§ ê°€ì§€ê³  ìˆìŒ)
- `_parse_stat()` â€” `bobaedream.py`, `dcinside.py`, `fmkorea.py`ì—ì„œ ì‚­ì œ
  - `nate_pann.py`ì˜ `_parse_stat`ì€ ì‹œê·¸ë‹ˆì²˜ê°€ ë‹¤ë¦„ (`soup, selector, prefix` â†’ ì‚­ì œí•˜ê³  í˜¸ì¶œë¶€ ìˆ˜ì •)

#### (C) `nate_pann.py`ì˜ `_parse_stat` íŠ¹ë³„ ì²˜ë¦¬

`nate_pann.py`ì˜ `_parse_stat`ì€ `(soup, selector, prefix)` ì‹œê·¸ë‹ˆì²˜ë¡œ BaseCrawler ê²ƒê³¼ ë‹¤ë¥´ë‹¤.
ì´ ë©”ì„œë“œëŠ” **ì‚­ì œí•˜ê³ **, í˜¸ì¶œë¶€ë¥¼ BaseCrawlerì˜ `_text()` + `_parse_int()` ì¡°í•©ìœ¼ë¡œ êµì²´í•˜ë¼:

```python
# Before
views = self._parse_stat(soup, "div.post-tit-info div.info span.count", prefix="ì¡°íšŒ")

# After
views_el = soup.select_one("div.post-tit-info div.info span.count")
views = self._parse_int(self._text(views_el).replace("ì¡°íšŒ", ""))
```

#### (D) `_text()` ë©”ì„œë“œ

`nate_pann.py`ì—ë§Œ ìˆë˜ `_text()` ì •ì  ë©”ì„œë“œëŠ” BaseCrawlerë¡œ ì´ë™í–ˆìœ¼ë¯€ë¡œ `nate_pann.py`ì—ì„œ ì‚­ì œí•˜ë¼.
ë‹¤ë¥¸ í¬ë¡¤ëŸ¬ì—ì„œë„ `el.get_text(strip=True) if el else ""` íŒ¨í„´ì´ ìˆìœ¼ë©´ `self._text(el)`ë¡œ êµì²´í•˜ë¼.

#### (E) HTTP ìš”ì²­ êµì²´ (ì„ íƒ â€” ì•ˆì „ ìš°ì„ ì´ë©´ ìŠ¤í‚µ ê°€ëŠ¥)

ê° í¬ë¡¤ëŸ¬ì˜ HTTP ìš”ì²­ íŒ¨í„´ì„ `self._get()` / `self._post()`ë¡œ êµì²´í•  ìˆ˜ ìˆë‹¤.
ë‹¨, ê¸°ì¡´ í¬ë¡¤ëŸ¬ì— `try/except`ë¡œ ì—ëŸ¬ ì²˜ë¦¬í•˜ëŠ” ë¶€ë¶„ì´ ìˆìœ¼ë¯€ë¡œ, **ë™ì‘ ë³€ê²½ ì—†ì´** êµì²´ê°€ ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ êµì²´í•˜ë¼.

êµì²´ ê°€ëŠ¥í•œ íŒ¨í„´:
```python
# Before
self._rotate_ua()
resp = self._session.get(url, timeout=REQUEST_TIMEOUT)
resp.raise_for_status()

# After
resp = self._get(url)
```

êµì²´ **ë¶ˆê°€ëŠ¥**í•œ íŒ¨í„´ (try/except ì•ˆì—ì„œ raise_for_status ì „ì— ë‹¤ë¥¸ ì²˜ë¦¬ê°€ ìˆëŠ” ê²½ìš°):
```python
# ì´ëŸ° ê²½ìš°ëŠ” ê·¸ëŒ€ë¡œ ë‘”ë‹¤
try:
    self._rotate_ua()
    resp = self._session.get(url, timeout=REQUEST_TIMEOUT)
    resp.raise_for_status()
except requests.RequestException:
    log.exception(...)
    continue  # ì´ continue ë•Œë¬¸ì— ë‹¨ìˆœ êµì²´ ë¶ˆê°€
```

â†’ `try/except` ì•ˆì—ì„œ `self._get()`ì„ ì“°ë©´ ë™ì¼í•˜ê²Œ ë™ì‘í•˜ë¯€ë¡œ êµì²´ ê°€ëŠ¥. ë‹¨, `_get()`ì´ ë‚´ë¶€ì—ì„œ `raise_for_status()`ë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ê¸°ì¡´ `resp.raise_for_status()` ì¤„ì€ ì‚­ì œí•´ì•¼ í•œë‹¤.

### 2-4. import ì •ë¦¬

ê° í¬ë¡¤ëŸ¬ íŒŒì¼ì—ì„œ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” importë¥¼ ì œê±°í•˜ë¼:
- `random` â€” `_rotate_ua` ì‚­ì œ ì‹œ (ë‹¨, ë‹¤ë¥¸ ê³³ì—ì„œ ì“°ëŠ”ì§€ í™•ì¸)
- `from config.settings import REQUEST_HEADERS, REQUEST_TIMEOUT, USER_AGENTS` â€” BaseCrawlerê°€ ì²˜ë¦¬í•˜ë¯€ë¡œ í¬ë¡¤ëŸ¬ì—ì„œ ì§ì ‘ ì‚¬ìš© ì•ˆ í•˜ë©´ ì œê±°
  - ë‹¨, `BOBAEDREAM_SECTIONS`, `DCINSIDE_SECTIONS` ë“± ì„¹ì…˜ ìƒìˆ˜ëŠ” ì•„ì§ settingsì—ì„œ importí•´ì•¼ í•˜ë¯€ë¡œ **Task 3 ì´í›„ì— ì •ë¦¬**

---

## Task 3: í¬ë¡¤ëŸ¬ ì„¹ì…˜ ì„¤ì •ì„ ê° í¬ë¡¤ëŸ¬ë¡œ ì´ë™

### 3-1. í˜„ì¬ ë¬¸ì œ

`config/settings.py`ì— 4ê°œ í¬ë¡¤ëŸ¬ì˜ URL ì„¹ì…˜ì´ í•˜ë“œì½”ë”©ë˜ì–´ ìˆë‹¤:
```python
NATE_PANN_SECTIONS = [...]
BOBAEDREAM_SECTIONS = [...]
DCINSIDE_SECTIONS = [...]
FMKOREA_SECTIONS = [...]
```

ìƒˆ í¬ë¡¤ëŸ¬ ì¶”ê°€ ì‹œ `settings.py`ë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ë©°, í¬ë¡¤ëŸ¬ì˜ ì‘ì§‘ë„ê°€ ë‚®ë‹¤.

### 3-2. ê° í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ì— `SECTIONS` í´ë˜ìŠ¤ ë³€ìˆ˜ ì¶”ê°€

ê° í¬ë¡¤ëŸ¬ íŒŒì¼ì—ì„œ settings import ëŒ€ì‹  í´ë˜ìŠ¤ ë‚´ë¶€ì— ì •ì˜í•˜ë¼.

**nate_pann.py:**
```python
class NatePannCrawler(BaseCrawler):
    site_code = "nate_pann"
    SECTIONS = [
        {"name": "í†¡í†¡ ë² ìŠ¤íŠ¸", "url": "https://pann.nate.com/talk/ranking"},
        {"name": "í†¡ì»¤ë“¤ì˜ ì„ íƒ", "url": "https://pann.nate.com/talk/ranking/best"},
    ]
```

**bobaedream.py:**
```python
class BobaedreamCrawler(BaseCrawler):
    site_code = "bobaedream"
    SECTIONS = [
        {"name": "ììœ ê²Œì‹œíŒ ë² ìŠ¤íŠ¸", "url": "https://m.bobaedream.co.kr/board/best/freeb"},
        {"name": "ì „ì²´ ë² ìŠ¤íŠ¸", "url": "https://m.bobaedream.co.kr/board/new_writing/best"},
    ]
```

**dcinside.py:**
```python
class DcInsideCrawler(BaseCrawler):
    site_code = "dcinside"
    SECTIONS = [
        {"name": "ì‹¤ì‹œê°„ ë² ìŠ¤íŠ¸ (ì‹¤ë² )", "url": "https://gall.dcinside.com/board/lists/?id=dcbest"},
        {"name": "HIT ê°¤ëŸ¬ë¦¬ (í›ê°¤)", "url": "https://gall.dcinside.com/board/lists/?id=hit"},
    ]
```

**fmkorea.py:**
```python
class FMKoreaCrawler(BaseCrawler):
    site_code = "fmkorea"
    SECTIONS = [
        {"name": "í¬í… í„°ì§ ìµœì‹ ìˆœ", "url": "https://www.fmkorea.com/index.php?mid=best"},
        {"name": "í¬í… í„°ì§ í™”ì œìˆœ", "url": "https://www.fmkorea.com/index.php?mid=best2&sort_index=pop&order_type=desc"},
    ]
```

### 3-3. ê° í¬ë¡¤ëŸ¬ì˜ `fetch_listing()` ìˆ˜ì •

ê° íŒŒì¼ì—ì„œ `for section in XXX_SECTIONS:` â†’ `for section in self.SECTIONS:` ë¡œ êµì²´í•˜ë¼.

### 3-4. settings.pyì—ì„œ ì„¹ì…˜ ìƒìˆ˜ ì œê±°

`config/settings.py`ì—ì„œ ì•„ë˜ 4ê°œ ë³€ìˆ˜ ë¸”ë¡ì„ **ì‚­ì œ**í•˜ë¼:

```python
NATE_PANN_SECTIONS = [...]
BOBAEDREAM_SECTIONS = [...]
DCINSIDE_SECTIONS = [...]
FMKOREA_SECTIONS = [...]
```

### 3-5. import ì •ë¦¬

ê° í¬ë¡¤ëŸ¬ íŒŒì¼ì—ì„œ ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” settings importë¥¼ ì œê±°í•˜ë¼:
```python
# ì‚­ì œ ëŒ€ìƒ (ê° íŒŒì¼ì—ì„œ í•´ë‹¹ í•­ëª©ë§Œ)
from config.settings import NATE_PANN_SECTIONS   # nate_pann.py
from config.settings import BOBAEDREAM_SECTIONS   # bobaedream.py
from config.settings import DCINSIDE_SECTIONS     # dcinside.py
from config.settings import FMKOREA_SECTIONS      # fmkorea.py
```

ë‹¤ë¥¸ settings import (`REQUEST_TIMEOUT` ë“±)ëŠ” Task 2ì—ì„œ BaseCrawlerë¡œ ì´ë™í–ˆìœ¼ë¯€ë¡œ í¬ë¡¤ëŸ¬ì—ì„œ ì§ì ‘ ì‚¬ìš© ì•ˆ í•˜ë©´ í•¨ê»˜ ì œê±°.

---

## Task 4: DB ë§ˆì´ê·¸ë ˆì´ì…˜ ëŸ¬ë„ˆ í†µí•©

### 4-1. í˜„ì¬ ë¬¸ì œ

- `db/migrations/` ì— SQL íŒŒì¼ 2ê°œ + ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ 2ê°œê°€ ê±°ì˜ ë™ì¼í•œ ì½”ë“œë¡œ ì¡´ì¬
- `db/migrate_001_images_contents.sql`ì€ `db/migrations/` ë°–ì— ë”°ë¡œ ì¡´ì¬
- ì–´ë–¤ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì ìš©ë˜ì—ˆëŠ”ì§€ ì¶”ì  ë¶ˆê°€

### 4-2. ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ ì •ë¦¬

**íŒŒì¼ ì´ë™ ë° ì´ë¦„ ì •ê·œí™”:**
```bash
# migrate_001ì„ migrations ë””ë ‰í† ë¦¬ë¡œ ì´ë™ + ë„¤ì´ë° í†µì¼
mv db/migrate_001_images_contents.sql db/migrations/001_images_contents.sql
mv db/migrations/add_llm_logs.sql db/migrations/002_add_llm_logs.sql
mv db/migrations/add_variant_fields.sql db/migrations/003_add_variant_fields.sql
```

**ê°œë³„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‚­ì œ:**
```bash
rm db/migrations/run_llm_logs_migration.py
rm db/migrations/run_migration.py
```

### 4-3. í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜ ëŸ¬ë„ˆ ìƒì„±

`db/migrations/runner.py` íŒŒì¼ì„ ìƒˆë¡œ ìƒì„±í•˜ë¼:

```python
"""í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜ ëŸ¬ë„ˆ.

ì ìš©ë˜ì§€ ì•Šì€ SQL ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•œë‹¤.
schema_migrations í…Œì´ë¸”ë¡œ ì ìš© ì´ë ¥ì„ ì¶”ì í•œë‹¤.

ì‚¬ìš©ë²•:
    docker compose exec dashboard python -m db.migrations.runner
"""

import logging
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))

from sqlalchemy import text
from db.session import engine

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(message)s")

MIGRATIONS_DIR = Path(__file__).parent


def _ensure_tracking_table(conn) -> None:
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ì¶”ì  í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±í•œë‹¤."""
    conn.execute(text("""
        CREATE TABLE IF NOT EXISTS schema_migrations (
            version   VARCHAR(64)  PRIMARY KEY,
            applied_at DATETIME    NOT NULL DEFAULT CURRENT_TIMESTAMP
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
    """))
    conn.commit()


def _get_applied(conn) -> set[str]:
    """ì´ë¯¸ ì ìš©ëœ ë§ˆì´ê·¸ë ˆì´ì…˜ ë²„ì „ ëª©ë¡ì„ ë°˜í™˜í•œë‹¤."""
    rows = conn.execute(text("SELECT version FROM schema_migrations")).fetchall()
    return {row[0] for row in rows}


def _run_sql(conn, sql_text: str) -> None:
    """ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë¶„ë¦¬ëœ SQL ë¬¸ì„ ìˆœì°¨ ì‹¤í–‰í•œë‹¤."""
    for stmt in sql_text.split(";"):
        lines = [
            ln for ln in stmt.splitlines()
            if ln.strip() and not ln.strip().startswith("--")
        ]
        if not lines:
            continue
        conn.execute(text("\n".join(lines)))


def migrate() -> None:
    """ë¯¸ì ìš© ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ìˆœì°¨ ì‹¤í–‰í•œë‹¤."""
    sql_files = sorted(MIGRATIONS_DIR.glob("*.sql"))

    if not sql_files:
        logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ ì—†ìŒ")
        return

    with engine.connect() as conn:
        _ensure_tracking_table(conn)
        applied = _get_applied(conn)

        for sql_file in sql_files:
            version = sql_file.stem  # e.g. "001_images_contents"

            if version in applied:
                logger.info("â­  %s (ì´ë¯¸ ì ìš©ë¨)", version)
                continue

            logger.info("â–¶  %s ì ìš© ì¤‘...", version)
            sql_text = sql_file.read_text(encoding="utf-8")

            try:
                _run_sql(conn, sql_text)
                conn.execute(
                    text("INSERT INTO schema_migrations (version) VALUES (:v)"),
                    {"v": version},
                )
                conn.commit()
                logger.info("âœ… %s ì™„ë£Œ", version)
            except Exception as e:
                conn.rollback()
                logger.error("âŒ %s ì‹¤íŒ¨: %s", version, e)
                raise

    logger.info("ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")


if __name__ == "__main__":
    migrate()
```

### 4-4. `__init__.py` ìƒì„±

`db/migrations/__init__.py` íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ íŒŒì¼ë¡œ ìƒì„±í•˜ë¼.

### 4-5. README ì—…ë°ì´íŠ¸ ê²€í† 

`README.md`ì— DB ì´ˆê¸°í™” ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤. ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ëª…ë ¹ì„ ì¶”ê°€í•˜ë¼:

ê¸°ì¡´ DB ì´ˆê¸°í™” ì„¹ì…˜ ì•„ë˜ì— ì¶”ê°€:
```markdown
### 6. DB ë§ˆì´ê·¸ë ˆì´ì…˜ (ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì‹œ)

ì‹ ê·œ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì¶”ê°€ëœ ê²½ìš° ì•„ë˜ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
ì´ë¯¸ ì ìš©ëœ ë§ˆì´ê·¸ë ˆì´ì…˜ì€ ìë™ìœ¼ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.

\```bash
docker compose exec dashboard python -m db.migrations.runner
\```
```

---

## Task 5: ê²€ì¦

ëª¨ë“  ì‘ì—… ì™„ë£Œ í›„ ì•„ë˜ í•­ëª©ì„ í™•ì¸í•˜ë¼.

### 5-1. ì‚­ì œ íŒŒì¼ ì°¸ì¡° ê²€ì¦

```bash
# ì‚­ì œëœ íŒŒì¼ì„ ì°¸ì¡°í•˜ëŠ” ê³³ì´ ì—†ì–´ì•¼ í•¨
grep -r "configurable_crawler\|site_loader\|nate_tok\|sites\.yaml" --include="*.py" .
grep -r "NATE_PANN_SECTIONS\|BOBAEDREAM_SECTIONS\|DCINSIDE_SECTIONS\|FMKOREA_SECTIONS" --include="*.py" .
grep -r "run_migration\.py\|run_llm_logs_migration\.py" --include="*.py" .
```

ìœ„ ëª…ë ¹ ê²°ê³¼ê°€ ëª¨ë‘ ë¹„ì–´ìˆì–´ì•¼ í•œë‹¤. (ì´ ì§€ì‹œì„œ íŒŒì¼ê³¼ arch/ ë¬¸ì„œëŠ” ì œì™¸)

### 5-2. import ê²€ì¦

```bash
# ê° í¬ë¡¤ëŸ¬ê°€ ì •ìƒ import ë˜ëŠ”ì§€ í™•ì¸
python -c "from crawlers.nate_pann import NatePannCrawler; print('nate_pann OK')"
python -c "from crawlers.bobaedream import BobaedreamCrawler; print('bobaedream OK')" 2>&1 | head -3
python -c "from crawlers.dcinside import DcInsideCrawler; print('dcinside OK')" 2>&1 | head -3
python -c "from crawlers.fmkorea import FMKoreaCrawler; print('fmkorea OK')" 2>&1 | head -3
python -c "from crawlers.base import BaseCrawler; print('base OK')"
```

### 5-3. BaseCrawler ìƒì† ê²€ì¦

```bash
python -c "
from crawlers.nate_pann import NatePannCrawler
c = NatePannCrawler()
assert hasattr(c, '_get'), '_get ë©”ì„œë“œ ì—†ìŒ'
assert hasattr(c, '_parse_int'), '_parse_int ë©”ì„œë“œ ì—†ìŒ'
assert hasattr(c, '_rotate_ua'), '_rotate_ua ë©”ì„œë“œ ì—†ìŒ'
assert hasattr(c, '_session'), '_session ì—†ìŒ'
assert c._parse_int('1,234ëª…') == 1234, '_parse_int ê²°ê³¼ ë¶ˆì¼ì¹˜'
print('BaseCrawler ìƒì† ê²€ì¦ í†µê³¼')
"
```

### 5-4. ë§ˆì´ê·¸ë ˆì´ì…˜ ëŸ¬ë„ˆ ê²€ì¦

```bash
python -c "from db.migrations.runner import migrate; print('runner import OK')"
```

### 5-5. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸

```bash
# ì‚­ì œë˜ì–´ì•¼ í•  íŒŒì¼ â€” ëª¨ë‘ ì—†ì–´ì•¼ í•¨
test ! -f crawlers/configurable_crawler.py && echo "PASS" || echo "FAIL: configurable_crawler.py ì¡´ì¬"
test ! -f crawlers/site_loader.py && echo "PASS" || echo "FAIL: site_loader.py ì¡´ì¬"
test ! -f config/sites.yaml && echo "PASS" || echo "FAIL: sites.yaml ì¡´ì¬"
test ! -f crawlers/nate_tok.py && echo "PASS" || echo "FAIL: nate_tok.py ì¡´ì¬"
test ! -f db/migrations/run_migration.py && echo "PASS" || echo "FAIL: run_migration.py ì¡´ì¬"
test ! -f db/migrations/run_llm_logs_migration.py && echo "PASS" || echo "FAIL: run_llm_logs_migration.py ì¡´ì¬"

# ìƒì„±/ì´ë™ë˜ì–´ì•¼ í•  íŒŒì¼ â€” ëª¨ë‘ ìˆì–´ì•¼ í•¨
test -f db/migrations/runner.py && echo "PASS" || echo "FAIL: runner.py ì—†ìŒ"
test -f db/migrations/001_images_contents.sql && echo "PASS" || echo "FAIL: 001 ì—†ìŒ"
test -f db/migrations/002_add_llm_logs.sql && echo "PASS" || echo "FAIL: 002 ì—†ìŒ"
test -f db/migrations/003_add_variant_fields.sql && echo "PASS" || echo "FAIL: 003 ì—†ìŒ"
test -f db/migrations/__init__.py && echo "PASS" || echo "FAIL: __init__.py ì—†ìŒ"
```

---

## ì ˆëŒ€ ìˆ˜ì • ê¸ˆì§€ (CLAUDE.md ì¤€ìˆ˜)

| ëŒ€ìƒ | ì´ìœ  |
|---|---|
| `db/models.py` | ìŠ¤í‚¤ë§ˆ ë³€ê²½ í•„ìš” ì‹œ ë³„ë„ ìŠ¹ì¸ |
| `.env` | ì‹œí¬ë¦¿ í¬í•¨ |
| `docker-compose.yml` | GPU ë§¤í•‘ ë¯¼ê° |
| `docker-compose.galaxybook.yml` | ë™ê¸°í™” í•„ìš” |
| `requirements.txt` | ì˜ì¡´ì„± ì¶©ëŒ ìœ„í—˜ |
| `h264_nvenc` ê´€ë ¨ ì½”ë“œ | VRAM ì°¨ë‹¨ |

---

## ë³€ê²½ íŒŒì¼ ìš”ì•½

| íŒŒì¼ | ì‘ì—… |
|---|---|
| `crawlers/configurable_crawler.py` | ğŸ—‘ ì‚­ì œ |
| `crawlers/site_loader.py` | ğŸ—‘ ì‚­ì œ |
| `config/sites.yaml` | ğŸ—‘ ì‚­ì œ |
| `crawlers/nate_tok.py` | ğŸ—‘ ì‚­ì œ |
| `db/migrations/run_migration.py` | ğŸ—‘ ì‚­ì œ |
| `db/migrations/run_llm_logs_migration.py` | ğŸ—‘ ì‚­ì œ |
| `db/migrate_001_images_contents.sql` | ğŸ“¦ ì´ë™ â†’ `db/migrations/001_images_contents.sql` |
| `db/migrations/add_llm_logs.sql` | ğŸ“ ì´ë¦„ ë³€ê²½ â†’ `002_add_llm_logs.sql` |
| `db/migrations/add_variant_fields.sql` | ğŸ“ ì´ë¦„ ë³€ê²½ â†’ `003_add_variant_fields.sql` |
| `crawlers/base.py` | âœï¸ ê³µí†µ í—¬í¼ ì¶”ê°€ |
| `crawlers/nate_pann.py` | âœï¸ ì¤‘ë³µ ì œê±° + ì„¹ì…˜ ì´ë™ |
| `crawlers/bobaedream.py` | âœï¸ ì¤‘ë³µ ì œê±° + ì„¹ì…˜ ì´ë™ |
| `crawlers/dcinside.py` | âœï¸ ì¤‘ë³µ ì œê±° + ì„¹ì…˜ ì´ë™ |
| `crawlers/fmkorea.py` | âœï¸ ì¤‘ë³µ ì œê±° + ì„¹ì…˜ ì´ë™ |
| `config/settings.py` | âœï¸ 4ê°œ ì„¹ì…˜ ìƒìˆ˜ ì œê±° |
| `db/migrations/runner.py` | ğŸ†• í†µí•© ë§ˆì´ê·¸ë ˆì´ì…˜ ëŸ¬ë„ˆ |
| `db/migrations/__init__.py` | ğŸ†• íŒ¨í‚¤ì§€ ì´ˆê¸°í™” |
| `crawlers/__init__.py` | âœï¸ ì°¸ì¡° ì •ë¦¬ (í•„ìš” ì‹œ) |
| `main.py` | âœï¸ site_loader ì°¸ì¡° ì œê±° (í•„ìš” ì‹œ) |
