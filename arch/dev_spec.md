# WaggleBot ê°œë°œ ëª…ì„¸ì„œ (Development Specification)

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1 ëª©í‘œ
ì¸ê¸° ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ì„ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ ì‡¼ì¸  ì˜ìƒ(9:16 ë¹„ìœ¨)ìœ¼ë¡œ ë³€í™˜ í›„ ìœ íŠœë¸Œ ë“±ì— ìë™ ì—…ë¡œë“œí•˜ëŠ” ì™„ì „ ìë™í™” íŒŒì´í”„ë¼ì¸ êµ¬ì¶•.

### 1.2 í•˜ë“œì›¨ì–´ í™˜ê²½
- **ë…¸ë“œ:** ë‹¨ì¼ Windows PC (WSL Ubuntu í™˜ê²½)
- **GPU:** NVIDIA RTX 3080 Ti (12GB VRAM)
- **ì—­í• :** í¬ë¡¤ë§, DB, AI ì¶”ë¡ , ì˜ìƒ ë Œë”ë§, ì—…ë¡œë“œ ì „ì²´ ë‹´ë‹¹
- **ì œì•½:** VRAM ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ìˆœì°¨ ì²˜ë¦¬ í•„ìˆ˜

### 1.3 ê¸°ìˆ  ìŠ¤íƒ
- **ì–¸ì–´:** Python 3.12
- **DB:** MariaDB 11.x + SQLAlchemy ORM
- **ì›¹ UI:** Streamlit
- **ì˜ìƒ:** FFmpeg (NVENC ê°€ì†)
- **ì»¨í…Œì´ë„ˆ:** Docker Compose with GPU support
- **LLM:** EEVE-Korean-10.8B / Llama-3.1-8B-Instruct-Ko (4-bit ì–‘ìí™”)
- **TTS:** Kokoro-82M / GPT-SoVITS / Edge-TTS

---

## 2. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 2.1 ì „ì²´ ë°ì´í„° íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scheduler  â”‚ 1hr  â”‚   Crawler    â”‚ DB   â”‚  Dashboard   â”‚
â”‚  (Cron)     â”‚â”€â”€â”€â”€â”€>â”‚ (ìˆ˜ì§‘/íŒŒì‹±)   â”‚â”€â”€â”€â”€â”€>â”‚  (ê²€ìˆ˜ UI)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                      â”‚
                            â”‚ COLLECTED            â”‚ APPROVED
                            â–¼                      â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚         MariaDB                 â”‚
                     â”‚  Posts / Comments / Contents    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                      
                            â”‚ 10ì´ˆ Polling (APPROVED ê°ì§€)
                            â–¼                      
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  AI Worker  â”‚
                     â”‚  (LLM/TTS/  â”‚
                     â”‚   Render)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ RENDERED
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Uploader   â”‚
                     â”‚  (YouTube)  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ìƒíƒœ ì „ì´ (State Transition)

```
COLLECTED â†’ APPROVED â†’ PROCESSING â†’ RENDERED â†’ UPLOADED
    â†“
DECLINED (ê±°ì ˆë¨)
```

- **COLLECTED:** í¬ë¡¤ëŸ¬ê°€ DBì— ì €ì¥ ì™„ë£Œ
- **APPROVED:** ëŒ€ì‹œë³´ë“œì—ì„œ ê´€ë¦¬ì ìŠ¹ì¸
- **PROCESSING:** AI ì›Œì»¤ê°€ ì²˜ë¦¬ ì¤‘ (LLM/TTS/ë Œë”ë§)
- **RENDERED:** ì˜ìƒ ìƒì„± ì™„ë£Œ (ì—…ë¡œë“œ ëŒ€ê¸°)
- **UPLOADED:** ìµœì¢… ì—…ë¡œë“œ ì™„ë£Œ
- **DECLINED:** ê´€ë¦¬ìê°€ ê±°ì ˆ (ì²˜ë¦¬ ì•ˆ í•¨)

### 2.3 ì»¨í…Œì´ë„ˆ êµ¬ì„±

```yaml
# docker-compose.yml êµ¬ì¡°
services:
  db:          # MariaDB 11.x
  crawler:     # 1ì‹œê°„ë§ˆë‹¤ ìë™ ì‹¤í–‰
  ai_worker:   # GPU ì‚¬ìš©, DB í´ë§
  dashboard:   # Streamlit UI (8501 í¬íŠ¸)
  
volumes:
  - mariadb_data  # DB ì˜êµ¬ ì €ì¥
  - ./media       # ì˜ìƒ/ì˜¤ë””ì˜¤ íŒŒì¼ ê³µìœ 
  - ./assets      # ë°°ê²½ ì˜ìƒ, í°íŠ¸ (ì½ê¸° ì „ìš©)
```

---

## 3. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

### 3.1 í…Œì´ë¸”: posts

| í•„ë“œ | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| id | BIGINT (PK) | ìë™ ì¦ê°€ ID |
| site_code | VARCHAR(32) | ì‚¬ì´íŠ¸ ì½”ë“œ (ì˜ˆ: nate_pann, nate_tok) |
| origin_id | VARCHAR(64) UNIQUE | ì›ë³¸ ì‚¬ì´íŠ¸ì˜ ê²Œì‹œê¸€ ID |
| title | VARCHAR(512) | ê²Œì‹œê¸€ ì œëª© |
| content | TEXT | ë³¸ë¬¸ (HTML ì œê±°ë¨) |
| images | JSON | ì´ë¯¸ì§€ URL ë°°ì—´ `["url1", "url2"]` |
| stats | JSON | í†µê³„ `{"views": 1234, "likes": 567}` |
| status | ENUM | COLLECTED/APPROVED/PROCESSING/RENDERED/UPLOADED/DECLINED |
| created_at | DATETIME | ìµœì´ˆ ìˆ˜ì§‘ ì‹œê° |
| updated_at | DATETIME | ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê° |

**ì¸ë±ìŠ¤:**
- `site_code` (í¬ë¡¤ëŸ¬ í•„í„°ë§)
- `status` (AI ì›Œì»¤ í´ë§)
- `origin_id` UNIQUE (ì¤‘ë³µ ë°©ì§€)

### 3.2 í…Œì´ë¸”: comments

| í•„ë“œ | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| id | BIGINT (PK) | ìë™ ì¦ê°€ ID |
| post_id | BIGINT (FK) | ê²Œì‹œê¸€ ID |
| author | VARCHAR(128) | ì‘ì„±ì ë‹‰ë„¤ì„ |
| content | TEXT | ëŒ“ê¸€ ë‚´ìš© |
| content_hash | VARCHAR(64) | ì¤‘ë³µ ì²´í¬ìš© í•´ì‹œ |
| likes | INT | ì¶”ì²œìˆ˜ |

**ìš©ë„:** ë² ìŠ¤íŠ¸ ëŒ“ê¸€ì„ LLM ìš”ì•½ì— í¬í•¨

### 3.3 í…Œì´ë¸”: contents

| í•„ë“œ | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| id | BIGINT (PK) | ìë™ ì¦ê°€ ID |
| post_id | BIGINT UNIQUE (FK) | ê²Œì‹œê¸€ ID (1:1 ê´€ê³„) |
| summary_text | TEXT | LLM ìƒì„± ìš”ì•½ (200ì ë‚´ì™¸) |
| audio_path | VARCHAR(255) | TTS ìƒì„± ìŒì„± íŒŒì¼ ê²½ë¡œ |
| video_path | VARCHAR(255) | ìµœì¢… ë Œë”ë§ ì˜ìƒ ê²½ë¡œ |
| upload_meta | JSON | ì—…ë¡œë“œ ê²°ê³¼ `{"youtube_id": "xxx"}` |
| created_at | DATETIME | ìƒì„± ì‹œê° |

---

## 4. ëª¨ë“ˆë³„ ìƒì„¸ ëª…ì„¸

### 4.1 í¬ë¡¤ëŸ¬ (Crawler)

#### 4.1.1 ì„¤ê³„ ì›ì¹™: í™•ì¥ì„±

**ëª©í‘œ:** 100ê°œ ì´ìƒì˜ ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ ì§€ì›  
**íŒ¨í„´:** BaseCrawler ì¶”ìƒ í´ë˜ìŠ¤ + í”ŒëŸ¬ê·¸ì¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬

#### 4.1.2 BaseCrawler ì¸í„°í˜ì´ìŠ¤

```python
# crawlers/base.py
from abc import ABC, abstractmethod
from typing import List, Dict, Optional

class BaseCrawler(ABC):
    """ëª¨ë“  í¬ë¡¤ëŸ¬ê°€ ìƒì†ë°›ì•„ì•¼ í•˜ëŠ” ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, site_code: str):
        self.site_code = site_code
        self.logger = logging.getLogger(f"crawler.{site_code}")
    
    @abstractmethod
    def fetch_listing(self, page: int = 1) -> List[Dict[str, str]]:
        """
        ê²Œì‹œê¸€ ëª©ë¡ í˜ì´ì§€ì—ì„œ URL ì¶”ì¶œ
        
        Returns:
            [{"url": "https://...", "title": "..."}, ...]
        """
        pass
    
    @abstractmethod
    def parse_post(self, url: str) -> Dict:
        """
        ê°œë³„ ê²Œì‹œê¸€ íŒŒì‹±
        
        Returns:
            {
                "origin_id": "12345",
                "title": "ì œëª©",
                "content": "ë³¸ë¬¸ í…ìŠ¤íŠ¸",
                "images": ["url1", "url2"],
                "stats": {"views": 1234, "likes": 567},
                "best_comment": {"author": "...", "content": "..."}
            }
        """
        pass
    
    def save_to_db(self, data: Dict) -> bool:
        """
        DBì— Upsert (ì¤‘ë³µ ì‹œ í†µê³„ë§Œ ì—…ë°ì´íŠ¸)
        """
        with SessionLocal() as db:
            existing = db.query(Post).filter_by(
                site_code=self.site_code,
                origin_id=data['origin_id']
            ).first()
            
            if existing:
                # ê¸°ì¡´ ê¸€: í†µê³„ë§Œ ì—…ë°ì´íŠ¸
                existing.stats = data['stats']
                existing.updated_at = datetime.now()
                self.logger.info(f"Updated stats: {data['origin_id']}")
            else:
                # ì‹ ê·œ ê¸€: ì „ì²´ ì €ì¥
                post = Post(
                    site_code=self.site_code,
                    origin_id=data['origin_id'],
                    title=data['title'],
                    content=data['content'],
                    images=json.dumps(data.get('images', [])),
                    stats=json.dumps(data['stats']),
                    status='COLLECTED'
                )
                db.add(post)
                self.logger.info(f"New post: {data['origin_id']}")
                
                # ë² ìŠ¤íŠ¸ ëŒ“ê¸€ ì €ì¥
                if data.get('best_comment'):
                    comment = Comment(
                        post_id=post.id,
                        author=data['best_comment']['author'],
                        content=data['best_comment']['content'],
                        content_hash=hashlib.sha256(
                            data['best_comment']['content'].encode()
                        ).hexdigest()
                    )
                    db.add(comment)
            
            db.commit()
            return True
    
    def run(self, max_pages: int = 3):
        """í¬ë¡¤ë§ ì‹¤í–‰ (ì—¬ëŸ¬ í˜ì´ì§€)"""
        for page in range(1, max_pages + 1):
            try:
                posts = self.fetch_listing(page)
                for post_meta in posts:
                    try:
                        data = self.parse_post(post_meta['url'])
                        self.save_to_db(data)
                        time.sleep(1)  # Rate limiting
                    except Exception as e:
                        self.logger.exception(f"Parse error: {post_meta['url']}")
            except Exception as e:
                self.logger.exception(f"Fetch error: page {page}")
```

#### 4.1.3 ë„¤ì´íŠ¸íŒ êµ¬í˜„ ì˜ˆì‹œ

```python
# crawlers/nate.py
from crawlers.base import BaseCrawler
from bs4 import BeautifulSoup
import requests

class NatePannCrawler(BaseCrawler):
    BASE_URL = "https://pann.nate.com"
    
    def fetch_listing(self, page: int = 1) -> List[Dict[str, str]]:
        url = f"{self.BASE_URL}/talk/ranking?page={page}"
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        posts = []
        for item in soup.select('.list_item'):
            posts.append({
                'url': self.BASE_URL + item.select_one('a')['href'],
                'title': item.select_one('.tit').text.strip()
            })
        return posts
    
    def parse_post(self, url: str) -> Dict:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Origin ID ì¶”ì¶œ (URLì—ì„œ)
        origin_id = url.split('/')[-1]
        
        # ë³¸ë¬¸
        content_div = soup.select_one('.article_content')
        content = content_div.get_text(strip=True)
        
        # ì´ë¯¸ì§€ (ë³¸ë¬¸ ë‚´ img íƒœê·¸)
        images = [img['src'] for img in content_div.select('img') if img.get('src')]
        
        # í†µê³„
        views = int(soup.select_one('.view_count').text.replace(',', ''))
        likes = int(soup.select_one('.like_count').text.replace(',', ''))
        
        # ë² ìŠ¤íŠ¸ ëŒ“ê¸€ (ì¶”ì²œìˆ˜ 1ìœ„)
        comments = soup.select('.comment_item')
        best_comment = None
        if comments:
            sorted_comments = sorted(
                comments, 
                key=lambda c: int(c.select_one('.like_count').text or 0),
                reverse=True
            )
            best = sorted_comments[0]
            best_comment = {
                'author': best.select_one('.author').text.strip(),
                'content': best.select_one('.content').text.strip(),
                'likes': int(best.select_one('.like_count').text or 0)
            }
        
        return {
            'origin_id': origin_id,
            'title': soup.select_one('.article_title').text.strip(),
            'content': content,
            'images': images,
            'stats': {'views': views, 'likes': likes},
            'best_comment': best_comment
        }
```

#### 4.1.4 í”ŒëŸ¬ê·¸ì¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ (í™•ì¥ìš©)

```python
# crawlers/registry.py
from typing import Dict, Type
from crawlers.base import BaseCrawler

class CrawlerRegistry:
    _crawlers: Dict[str, Type[BaseCrawler]] = {}
    
    @classmethod
    def register(cls, site_code: str):
        def decorator(crawler_class):
            cls._crawlers[site_code] = crawler_class
            return crawler_class
        return decorator
    
    @classmethod
    def get(cls, site_code: str) -> BaseCrawler:
        if site_code not in cls._crawlers:
            raise ValueError(f"Unknown site: {site_code}")
        return cls._crawlers[site_code](site_code)

# ì‚¬ìš© ì˜ˆì‹œ
@CrawlerRegistry.register('nate_pann')
class NatePannCrawler(BaseCrawler):
    pass

# main.py
for site in ['nate_pann', 'nate_tok']:
    crawler = CrawlerRegistry.get(site)
    crawler.run(max_pages=3)
```

---

### 4.2 ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œ (Streamlit)

#### 4.2.1 UI êµ¬ì¡°

**3ê°œ íƒ­:**
1. **ìˆ˜ì‹ í•¨ (Inbox):** COLLECTED ìƒíƒœ ê²Œì‹œê¸€ ìŠ¹ì¸/ê±°ì ˆ
2. **ì§„í–‰ ìƒíƒœ (Progress):** PROCESSING/RENDERED/UPLOADED ëª¨ë‹ˆí„°ë§
3. **ê°¤ëŸ¬ë¦¬ (Gallery):** ì™„ì„±ëœ ì˜ìƒ ì¬ìƒ

#### 4.2.2 Tab 1: ìˆ˜ì‹ í•¨ êµ¬í˜„

```python
# dashboard.py
import streamlit as st
from streamlit_autorefresh import st_autorefresh
from db.session import SessionLocal
from db.models import Post, Comment

# 30ì´ˆë§ˆë‹¤ ìë™ ìƒˆë¡œê³ ì¹¨
st_autorefresh(interval=30000, key="refresh")

def render_inbox():
    st.header("ğŸ“¥ ìˆ˜ì‹ í•¨ (Collected)")
    
    # í•„í„°ë§ ì˜µì…˜
    col1, col2, col3 = st.columns(3)
    with col1:
        sites = st.multiselect("ì‚¬ì´íŠ¸", ["nate_pann", "nate_tok"], default=None)
    with col2:
        has_image = st.selectbox("ì´ë¯¸ì§€", ["ì „ì²´", "ìˆìŒ", "ì—†ìŒ"])
    with col3:
        sort_by = st.selectbox("ì •ë ¬", ["ìµœì‹ ìˆœ", "ì¡°íšŒìˆ˜ìˆœ", "ì¶”ì²œìˆ˜ìˆœ"])
    
    # ë°ì´í„° ì¡°íšŒ
    with SessionLocal() as db:
        query = db.query(Post).filter(Post.status == 'COLLECTED')
        
        # í•„í„° ì ìš©
        if sites:
            query = query.filter(Post.site_code.in_(sites))
        if has_image == "ìˆìŒ":
            query = query.filter(Post.images != '[]')
        elif has_image == "ì—†ìŒ":
            query = query.filter(Post.images == '[]')
        
        # ì •ë ¬
        if sort_by == "ì¡°íšŒìˆ˜ìˆœ":
            query = query.order_by(Post.stats['views'].desc())
        elif sort_by == "ì¶”ì²œìˆ˜ìˆœ":
            query = query.order_by(Post.stats['likes'].desc())
        else:
            query = query.order_by(Post.created_at.desc())
        
        posts = query.limit(50).all()
    
    # ê²Œì‹œê¸€ ì¹´ë“œ ë Œë”ë§
    for post in posts:
        with st.container():
            col1, col2 = st.columns([4, 1])
            
            with col1:
                st.markdown(f"### {post.title}")
                stats = json.loads(post.stats)
                st.caption(
                    f"ğŸŒ {post.site_code} | "
                    f"ğŸ‘ï¸ {stats.get('views', 0):,} | "
                    f"ğŸ‘ {stats.get('likes', 0):,}"
                )
                
                # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                with st.expander("ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"):
                    st.write(post.content[:300] + "...")
                    
                    # ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°
                    images = json.loads(post.images)
                    if images:
                        st.image(images[0], width=200, caption="ì²« ë²ˆì§¸ ì´ë¯¸ì§€")
                
                # ë² ìŠ¤íŠ¸ ëŒ“ê¸€
                comments = db.query(Comment).filter_by(post_id=post.id).all()
                if comments:
                    best = max(comments, key=lambda c: c.likes)
                    st.info(f"ğŸ’¬ **{best.author}:** {best.content[:100]}...")
            
            with col2:
                st.write("")  # ê°„ê²©
                if st.button("âœ… ìŠ¹ì¸", key=f"approve_{post.id}"):
                    with SessionLocal() as db:
                        db.query(Post).filter_by(id=post.id).update({
                            'status': 'APPROVED'
                        })
                        db.commit()
                    st.success("ìŠ¹ì¸ë¨")
                    st.rerun()
                
                if st.button("âŒ ê±°ì ˆ", key=f"decline_{post.id}"):
                    with SessionLocal() as db:
                        db.query(Post).filter_by(id=post.id).update({
                            'status': 'DECLINED'
                        })
                        db.commit()
                    st.warning("ê±°ì ˆë¨")
                    st.rerun()
            
            st.divider()
```

#### 4.2.3 Tab 2: ì§„í–‰ ìƒíƒœ

```python
def render_progress():
    st.header("âš™ï¸ ì§„í–‰ ìƒíƒœ")
    
    with SessionLocal() as db:
        counts = {
            'APPROVED': db.query(Post).filter_by(status='APPROVED').count(),
            'PROCESSING': db.query(Post).filter_by(status='PROCESSING').count(),
            'RENDERED': db.query(Post).filter_by(status='RENDERED').count(),
            'UPLOADED': db.query(Post).filter_by(status='UPLOADED').count(),
        }
    
    # ë©”íŠ¸ë¦­ í‘œì‹œ
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("ëŒ€ê¸°ì¤‘", counts['APPROVED'])
    col2.metric("ì²˜ë¦¬ì¤‘", counts['PROCESSING'])
    col3.metric("ë Œë”ë§ ì™„ë£Œ", counts['RENDERED'])
    col4.metric("ì—…ë¡œë“œ ì™„ë£Œ", counts['UPLOADED'])
    
    # ì²˜ë¦¬ ì¤‘ì¸ í•­ëª© ìƒì„¸
    st.subheader("ì²˜ë¦¬ ì¤‘ì¸ í•­ëª©")
    with SessionLocal() as db:
        processing = db.query(Post).filter_by(status='PROCESSING').all()
        for post in processing:
            st.write(f"- {post.title} (ID: {post.id})")
```

#### 4.2.4 Tab 3: ê°¤ëŸ¬ë¦¬

```python
def render_gallery():
    st.header("ğŸ¬ ê°¤ëŸ¬ë¦¬")
    
    with SessionLocal() as db:
        contents = db.query(Content).join(Post).filter(
            Post.status.in_(['RENDERED', 'UPLOADED'])
        ).order_by(Content.created_at.desc()).limit(20).all()
    
    # 3ì—´ ê·¸ë¦¬ë“œ
    cols = st.columns(3)
    for idx, content in enumerate(contents):
        with cols[idx % 3]:
            # ì˜ìƒ ì¬ìƒ
            video_path = f"/app/media/{content.video_path}"
            if os.path.exists(video_path):
                st.video(video_path)
                st.caption(content.post.title[:30] + "...")
                
                # ìš”ì•½ í…ìŠ¤íŠ¸
                with st.expander("ìš”ì•½"):
                    st.write(content.summary_text)
                
                # ì—…ë¡œë“œ ë²„íŠ¼
                if content.post.status == 'RENDERED':
                    if st.button("ğŸ“¤ ì—…ë¡œë“œ", key=f"upload_{content.id}"):
                        # ì—…ë¡œë” íŠ¸ë¦¬ê±° (ë³„ë„ êµ¬í˜„)
                        trigger_upload(content.id)
```

---

### 4.3 AI ì›Œì»¤ (LLM/TTS/Render)

#### 4.3.1 VRAM ê´€ë¦¬ í•µì‹¬ íŒ¨í„´

**ë¬¸ì œì :**
- RTX 3080 Ti 12GBëŠ” LLM(4GB) + TTS(2GB) + FFmpeg(2GB) ë™ì‹œ ë¡œë“œ ë¶ˆê°€ëŠ¥
- OOM ë°œìƒ ì‹œ ì»¨í…Œì´ë„ˆ í¬ë˜ì‹œ â†’ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨

**í•´ê²°ì±…:**
1. **ìˆœì°¨ ì²˜ë¦¬:** LLM â†’ TTS â†’ ë Œë”ë§ ë‹¨ê³„ë³„ ì‹¤í–‰
2. **ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ:** ê° ë‹¨ê³„ í›„ `torch.cuda.empty_cache()` + `gc.collect()`
3. **ëª¨ë¸ ì–¸ë¡œë“œ:** ë‹¤ìŒ ëª¨ë¸ ë¡œë“œ ì „ ì´ì „ ëª¨ë¸ ì™„ì „ ì‚­ì œ

```python
# ai_worker/gpu_manager.py
import torch
import gc
from contextlib import contextmanager
from typing import Literal

ModelType = Literal['llm', 'tts']

class GPUMemoryManager:
    def __init__(self):
        self.loaded_models = {}
        self.logger = logging.getLogger(__name__)
    
    @contextmanager
    def managed_inference(self, model_type: ModelType):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €ë¡œ GPU ë©”ëª¨ë¦¬ ìë™ ê´€ë¦¬"""
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ VRAM í™•ì¸
            available = self.get_available_vram()
            required = {'llm': 4.5, 'tts': 2.5}[model_type]
            
            if available < required:
                self.logger.warning(
                    f"Insufficient VRAM: {available:.1f}GB < {required}GB"
                )
                # ê¸°ì¡´ ëª¨ë¸ ì–¸ë¡œë“œ
                self.unload_all()
            
            self.logger.info(f"Loading {model_type} model...")
            yield
            
        finally:
            # ì¶”ë¡  ì™„ë£Œ í›„ ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
            torch.cuda.empty_cache()
            gc.collect()
            self.logger.info(f"Released {model_type} memory")
    
    def get_available_vram(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ VRAM (GB)"""
        if torch.cuda.is_available():
            free, total = torch.cuda.mem_get_info()
            return free / 1024**3
        return 0.0
    
    def unload_all(self):
        """ëª¨ë“  ëª¨ë¸ ì–¸ë¡œë“œ"""
        self.loaded_models.clear()
        torch.cuda.empty_cache()
        gc.collect()
        self.logger.info("Unloaded all models")
```

#### 4.3.2 AI ì›Œì»¤ ë©”ì¸ ë£¨í”„

```python
# ai_worker/main.py
import time
from ai_worker.gpu_manager import GPUMemoryManager
from ai_worker.llm import LLMSummarizer
from ai_worker.tts import TTSGenerator
from ai_worker.renderer import VideoRenderer

class AIWorker:
    def __init__(self):
        self.gpu_manager = GPUMemoryManager()
        self.llm = None
        self.tts = None
        self.renderer = VideoRenderer()
        self.logger = logging.getLogger(__name__)
    
    def poll_and_process(self):
        """DBì—ì„œ APPROVED ìƒíƒœ í´ë§ (10ì´ˆ ê°„ê²©)"""
        while True:
            try:
                with SessionLocal() as db:
                    # Race condition ë°©ì§€: SELECT FOR UPDATE SKIP LOCKED
                    post = db.query(Post).filter_by(status='APPROVED').with_for_update(skip_locked=True).first()
                    
                    if post:
                        # ìƒíƒœë¥¼ ì¦‰ì‹œ PROCESSINGìœ¼ë¡œ ë³€ê²½ (ë‹¤ë¥¸ ì›Œì»¤ ì¤‘ë³µ ë°©ì§€)
                        post.status = 'PROCESSING'
                        db.commit()
                        post_id = post.id
                
                if post:
                    self.logger.info(f"Processing post: {post_id}")
                    success = self.process_post(post_id)
                    
                    if success:
                        self.update_status(post_id, 'RENDERED')
                    else:
                        self.update_status(post_id, 'FAILED')
                
                time.sleep(10)  # 10ì´ˆ ëŒ€ê¸°
                
            except Exception as e:
                self.logger.exception("Polling error")
                time.sleep(30)  # ì—ëŸ¬ ì‹œ ë” ê¸´ ëŒ€ê¸°
    
    def process_post(self, post_id: int) -> bool:
        """
        3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸: LLM â†’ TTS â†’ Render
        ê° ë‹¨ê³„ í›„ GPU ë©”ëª¨ë¦¬ ëª…ì‹œì  í•´ì œ
        """
        try:
            # Step 1: LLM ìš”ì•½
            with self.gpu_manager.managed_inference('llm'):
                if not self.llm:
                    self.llm = LLMSummarizer()
                summary = self.llm.generate_summary(post_id)
                self.logger.info(f"Summary generated: {len(summary)} chars")
            
            # LLM ëª¨ë¸ ì–¸ë¡œë“œ (VRAM í™•ë³´)
            del self.llm
            self.llm = None
            torch.cuda.empty_cache()
            gc.collect()
            
            # Step 2: TTS ìƒì„±
            with self.gpu_manager.managed_inference('tts'):
                if not self.tts:
                    self.tts = TTSGenerator()
                audio_path = self.tts.generate_audio(summary, post_id)
                self.logger.info(f"Audio saved: {audio_path}")
            
            # TTS ëª¨ë¸ ì–¸ë¡œë“œ
            del self.tts
            self.tts = None
            torch.cuda.empty_cache()
            gc.collect()
            
            # Step 3: ì˜ìƒ ë Œë”ë§ (FFmpeg, NVENC ì‚¬ìš©)
            video_path = self.renderer.create_video(post_id, summary, audio_path)
            self.logger.info(f"Video rendered: {video_path}")
            
            # DBì— ê²°ê³¼ ì €ì¥
            self.save_content(post_id, summary, audio_path, video_path)
            return True
            
        except Exception as e:
            self.logger.exception(f"Processing failed: {post_id}")
            return False
    
    def save_content(self, post_id: int, summary: str, audio: str, video: str):
        """ì²˜ë¦¬ ê²°ê³¼ë¥¼ contents í…Œì´ë¸”ì— ì €ì¥"""
        with SessionLocal() as db:
            content = Content(
                post_id=post_id,
                summary_text=summary,
                audio_path=audio,
                video_path=video
            )
            db.add(content)
            db.commit()
    
    def update_status(self, post_id: int, status: str):
        """ê²Œì‹œê¸€ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        with SessionLocal() as db:
            db.query(Post).filter_by(id=post_id).update({'status': status})
            db.commit()

if __name__ == '__main__':
    worker = AIWorker()
    worker.poll_and_process()
```

#### 4.3.3 LLM ìš”ì•½ê¸°

```python
# ai_worker/llm.py
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

class LLMSummarizer:
    def __init__(self, model_name: str = "yanolja/EEVE-Korean-10.8B-v1.0"):
        self.logger = logging.getLogger(__name__)
        
        # 4-bit ì–‘ìí™” ì„¤ì • (VRAM ì ˆì•½)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        self.logger.info(f"LLM loaded: {model_name} (4-bit)")
    
    def generate_summary(self, post_id: int) -> str:
        """ê²Œì‹œê¸€ + ë² ìŠ¤íŠ¸ ëŒ“ê¸€ì„ 200ì ì‡¼ì¸  ëŒ€ë³¸ìœ¼ë¡œ ìš”ì•½"""
        with SessionLocal() as db:
            post = db.query(Post).filter_by(id=post_id).first()
            comments = db.query(Comment).filter_by(post_id=post_id).order_by(Comment.likes.desc()).limit(1).all()
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¤ìŒ ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ì„ ìœ íŠœë¸Œ ì‡¼ì¸ ìš© ëŒ€ë³¸ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
ì¡°ê±´:
- 200ì ì´ë‚´
- êµ¬ì–´ì²´ ì‚¬ìš©
- í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ
- ë² ìŠ¤íŠ¸ ëŒ“ê¸€ ë°˜ì‘ í¬í•¨

ì œëª©: {post.title}

ë³¸ë¬¸:
{post.content[:500]}

ë² ìŠ¤íŠ¸ ëŒ“ê¸€:
{comments[0].content if comments else 'ì—†ìŒ'}

ì‡¼ì¸  ëŒ€ë³¸:"""
        
        # ì¶”ë¡ 
        inputs = self.tokenizer(prompt, return_tensors="pt").to("cuda")
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                top_p=0.9
            )
        
        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
        summary = summary.split("ì‡¼ì¸  ëŒ€ë³¸:")[-1].strip()
        
        # 200ì ì œí•œ
        return summary[:200]
```

#### 4.3.4 TTS ìƒì„±ê¸°

```python
# ai_worker/tts.py
from pathlib import Path
import edge_tts
import asyncio

class TTSGenerator:
    def __init__(self, engine: str = "edge-tts"):
        self.engine = engine
        self.logger = logging.getLogger(__name__)
    
    def generate_audio(self, text: str, post_id: int) -> str:
        """í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
        output_path = Path(f"/app/media/audio/post_{post_id}.wav")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        if self.engine == "edge-tts":
            asyncio.run(self._edge_tts(text, output_path))
        elif self.engine == "kokoro":
            self._kokoro_tts(text, output_path)
        else:
            raise ValueError(f"Unknown TTS engine: {self.engine}")
        
        self.logger.info(f"TTS generated: {output_path}")
        return str(output_path)
    
    async def _edge_tts(self, text: str, output_path: Path):
        """Edge-TTS (ë¬´ë£Œ, ë¹ ë¦„)"""
        communicate = edge_tts.Communicate(text, "ko-KR-SunHiNeural")
        await communicate.save(str(output_path))
    
    def _kokoro_tts(self, text: str, output_path: Path):
        """Kokoro-82M (ë¡œì»¬, ê³ í’ˆì§ˆ)"""
        # TODO: Kokoro ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
        pass
```

#### 4.3.5 ì˜ìƒ ë Œë”ëŸ¬

```python
# ai_worker/renderer.py
from moviepy.editor import (
    VideoFileClip, ImageClip, AudioFileClip, TextClip, CompositeVideoClip
)
from pathlib import Path
import json

class VideoRenderer:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.backgrounds = list(Path("/app/assets/backgrounds").glob("*.mp4"))
    
    def create_video(self, post_id: int, summary: str, audio_path: str) -> str:
        """ì‡¼ì¸  ì˜ìƒ ìƒì„± (9:16 ë¹„ìœ¨)"""
        # ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
        audio = AudioFileClip(audio_path)
        duration = audio.duration
        
        # ê²Œì‹œê¸€ ì´ë¯¸ì§€ í™•ì¸
        with SessionLocal() as db:
            post = db.query(Post).filter_by(id=post_id).first()
            images = json.loads(post.images)
        
        if images:
            # ì´ë¯¸ì§€ ê¸°ë°˜ ìŠ¬ë¼ì´ë“œ ì‡¼
            video = self._create_slideshow(images, duration)
        else:
            # ë°°ê²½ ì˜ìƒ ì‚¬ìš©
            video = self._create_background_video(duration)
        
        # ìë§‰ ì¶”ê°€
        video_with_text = self._add_subtitles(video, summary)
        
        # ì˜¤ë””ì˜¤ í•©ì„± (TTS + BGM)
        final_video = video_with_text.set_audio(audio)
        
        # NVENC ì¸ì½”ë”© (GPU ê°€ì†)
        output_path = Path(f"/app/media/videos/post_{post_id}.mp4")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        final_video.write_videofile(
            str(output_path),
            codec='h264_nvenc',  # í•„ìˆ˜: GPU ê°€ì†
            audio_codec='aac',
            fps=30,
            preset='fast',
            ffmpeg_params=['-gpu', '0']  # RTX 3080 Ti ì§€ì •
        )
        
        self.logger.info(f"Video rendered: {output_path}")
        return str(output_path)
    
    def _create_slideshow(self, image_urls: list, duration: float) -> VideoFileClip:
        """ì´ë¯¸ì§€ ìŠ¬ë¼ì´ë“œ ì‡¼ (Ken Burns íš¨ê³¼)"""
        clips = []
        per_image_duration = duration / len(image_urls)
        
        for url in image_urls:
            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° 9:16 í¬ë¡­
            img_path = self._download_and_crop(url)
            
            # Ken Burns íš¨ê³¼ (ì¤Œì¸)
            clip = (ImageClip(img_path, duration=per_image_duration)
                    .resize(height=1920)
                    .set_position('center')
                    .crossfadein(0.5))
            clips.append(clip)
        
        return concatenate_videoclips(clips, method="compose")
    
    def _create_background_video(self, duration: float) -> VideoFileClip:
        """ë°°ê²½ ì˜ìƒ ë°˜ë³µ/ìë¦„"""
        bg = VideoFileClip(str(self.backgrounds[0]))
        
        if bg.duration < duration:
            # ë°˜ë³µ
            loops = int(duration / bg.duration) + 1
            bg = concatenate_videoclips([bg] * loops)
        
        # ê¸¸ì´ ìë¦„
        return bg.subclip(0, duration)
    
    def _add_subtitles(self, video: VideoFileClip, text: str) -> CompositeVideoClip:
        """ìë§‰ ì¶”ê°€ (í™”ë©´ ì¤‘ì•™)"""
        txt_clip = (TextClip(
            text,
            fontsize=60,
            color='white',
            font='NanumGothic-Bold',  # í•œê¸€ í°íŠ¸
            size=(1080, None),
            method='caption',
            align='center'
        ).set_position('center')
          .set_duration(video.duration))
        
        return CompositeVideoClip([video, txt_clip])
    
    def _download_and_crop(self, url: str) -> str:
        """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° 9:16 í¬ë¡­"""
        # TODO: requestsë¡œ ë‹¤ìš´ë¡œë“œ í›„ PILë¡œ í¬ë¡­
        pass
```

---

### 4.4 ì—…ë¡œë” (YouTube)

#### 4.4.1 í™•ì¥ ê°€ëŠ¥í•œ ì—…ë¡œë” íŒ¨í„´

```python
# uploaders/base.py
from abc import ABC, abstractmethod

class BaseUploader(ABC):
    @abstractmethod
    def upload(self, video_path: str, metadata: dict) -> dict:
        """
        ì˜ìƒ ì—…ë¡œë“œ
        
        Args:
            video_path: ì˜ìƒ íŒŒì¼ ê²½ë¡œ
            metadata: {title, description, tags, privacy}
        
        Returns:
            {platform_id, url}
        """
        pass
```

#### 4.4.2 YouTube ì—…ë¡œë”

```python
# uploaders/youtube.py
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

class YouTubeUploader(BaseUploader):
    def __init__(self, credentials_path: str):
        self.creds = Credentials.from_authorized_user_file(credentials_path)
        self.youtube = build('youtube', 'v3', credentials=self.creds)
    
    def upload(self, video_path: str, metadata: dict) -> dict:
        """YouTube Shorts ì—…ë¡œë“œ"""
        body = {
            'snippet': {
                'title': metadata['title'][:100],  # 100ì ì œí•œ
                'description': metadata['description'],
                'tags': metadata.get('tags', []),
                'categoryId': '22'  # People & Blogs
            },
            'status': {
                'privacyStatus': metadata.get('privacy', 'unlisted'),
                'selfDeclaredMadeForKids': False
            }
        }
        
        media = MediaFileUpload(
            video_path,
            chunksize=-1,
            resumable=True,
            mimetype='video/mp4'
        )
        
        request = self.youtube.videos().insert(
            part='snippet,status',
            body=body,
            media_body=media
        )
        
        response = request.execute()
        
        return {
            'platform_id': response['id'],
            'url': f"https://youtube.com/shorts/{response['id']}"
        }
```

---

## 5. Docker êµ¬ì„±

### 5.1 docker-compose.yml

```yaml
version: '3.8'

services:
  db:
    image: mariadb:11
    environment:
      MYSQL_ROOT_PASSWORD: ${DB_ROOT_PASSWORD}
      MYSQL_DATABASE: wagglebot
      MYSQL_USER: ${DB_USER}
      MYSQL_PASSWORD: ${DB_PASSWORD}
    volumes:
      - mariadb_data:/var/lib/mysql
      - ./db/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  crawler:
    build: .
    command: python scheduler.py
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - ./config:/app/config:ro
      - ./crawlers:/app/crawlers
    environment:
      DB_HOST: db
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
    restart: unless-stopped

  ai_worker:
    build: .
    command: python ai_worker/main.py
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - ./media:/app/media
      - ./assets:/app/assets:ro
      - ./config:/app/config:ro
      - models_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      CUDA_VISIBLE_DEVICES: 0
      DB_HOST: db
    restart: unless-stopped

  dashboard:
    build: .
    command: streamlit run dashboard.py --server.port=8501 --server.address=0.0.0.0
    depends_on:
      db:
        condition: service_healthy
    ports:
      - "8501:8501"
    volumes:
      - ./media:/app/media:ro
    environment:
      DB_HOST: db
    restart: unless-stopped

volumes:
  mariadb_data:
  models_cache:
```

### 5.2 Dockerfile

```dockerfile
FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04

# Python 3.12 ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3.12 python3-pip \
    ffmpeg \
    fonts-nanum \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µì‚¬
COPY . .

CMD ["python3", "-u", "main.py"]
```

---

## 6. ì—ëŸ¬ í•¸ë“¤ë§ ë° ë³µêµ¬

### 6.1 ì¬ì‹œë„ ë¡œì§

```python
# utils/retry.py
import time
import logging
from functools import wraps

def retry(max_attempts=3, backoff_factor=2, exceptions=(Exception,)):
    """ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            logger = logging.getLogger(func.__module__)
            
            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    if attempt == max_attempts:
                        logger.error(f"Final attempt failed: {func.__name__}")
                        raise
                    
                    wait_time = backoff_factor ** attempt
                    logger.warning(
                        f"Attempt {attempt}/{max_attempts} failed: {e}. "
                        f"Retrying in {wait_time}s..."
                    )
                    time.sleep(wait_time)
        
        return wrapper
    return decorator

# ì‚¬ìš© ì˜ˆì‹œ
@retry(max_attempts=3, exceptions=(requests.RequestException,))
def fetch_post(url):
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    return response.text
```

### 6.2 ì—ëŸ¬ ë¡œê¹…

```python
# utils/logger.py
import logging
from logging.handlers import RotatingFileHandler

def setup_logger(name: str, log_file: str = None):
    """êµ¬ì¡°í™”ëœ ë¡œê±° ìƒì„±"""
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    
    # í¬ë§·
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (10MB ë¡œí…Œì´ì…˜)
    if log_file:
        file_handler = RotatingFileHandler(
            log_file,
            maxBytes=10*1024*1024,
            backupCount=5
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger
```

---

## 7. í…ŒìŠ¤íŠ¸

### 7.1 ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
# tests/test_crawler.py
import pytest
from crawlers.nate import NatePannCrawler

@pytest.fixture
def crawler():
    return NatePannCrawler(site_code='nate_pann')

def test_fetch_listing(crawler):
    posts = crawler.fetch_listing(page=1)
    assert len(posts) > 0
    assert 'url' in posts[0]
    assert 'title' in posts[0]

def test_parse_post(crawler):
    # ì‹¤ì œ ê²Œì‹œê¸€ URL (í…ŒìŠ¤íŠ¸ ì‹œì ì— ìœ íš¨í•œ ê²ƒ)
    url = "https://pann.nate.com/talk/123456"
    data = crawler.parse_post(url)
    
    assert 'origin_id' in data
    assert 'title' in data
    assert 'content' in data
    assert isinstance(data['images'], list)
```

### 7.2 í†µí•© í…ŒìŠ¤íŠ¸

```python
# tests/test_pipeline.py
def test_end_to_end_pipeline(db_session):
    """í¬ë¡¤ë§ â†’ ìŠ¹ì¸ â†’ AI ì²˜ë¦¬ â†’ ì—…ë¡œë“œ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    # 1. í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = NatePannCrawler('nate_pann')
    crawler.run(max_pages=1)
    
    # 2. DB í™•ì¸
    post = db_session.query(Post).filter_by(status='COLLECTED').first()
    assert post is not None
    
    # 3. ìŠ¹ì¸
    post.status = 'APPROVED'
    db_session.commit()
    
    # 4. AI ì›Œì»¤ ì‹¤í–‰ (ëª¨í‚¹)
    worker = AIWorker()
    success = worker.process_post(post.id)
    
    assert success
    assert post.status == 'RENDERED'
    
    # 5. ê²°ê³¼ í™•ì¸
    content = db_session.query(Content).filter_by(post_id=post.id).first()
    assert content is not None
    assert content.video_path is not None
```

---

## 8. ì„±ëŠ¥ ìµœì í™”

### 8.1 DB ì¸ë±ìŠ¤

```sql
-- í¬ë¡¤ëŸ¬ í•„í„°ë§
CREATE INDEX idx_site_status ON posts(site_code, status);

-- AI ì›Œì»¤ í´ë§
CREATE INDEX idx_status_created ON posts(status, created_at);

-- ëŒ€ì‹œë³´ë“œ ì •ë ¬
CREATE INDEX idx_stats_views ON posts((stats->>'$.views'));
CREATE INDEX idx_stats_likes ON posts((stats->>'$.likes'));
```

### 8.2 ìºì‹± ì „ëµ

```python
# utils/cache.py
from functools import lru_cache
from typing import List

@lru_cache(maxsize=100)
def get_background_videos() -> List[str]:
    """ë°°ê²½ ì˜ìƒ ëª©ë¡ ìºì‹± (íŒŒì¼ ì‹œìŠ¤í…œ I/O ê°ì†Œ)"""
    return list(Path("/app/assets/backgrounds").glob("*.mp4"))
```

---

## 9. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 9.1 í—¬ìŠ¤ì²´í¬

```python
# utils/health.py
import psutil
import GPUtil

def check_system_health() -> dict:
    """ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬"""
    gpus = GPUtil.getGPUs()
    
    return {
        'cpu_percent': psutil.cpu_percent(interval=1),
        'memory_percent': psutil.virtual_memory().percent,
        'disk_percent': psutil.disk_usage('/').percent,
        'gpu_temp': gpus[0].temperature if gpus else None,
        'gpu_memory_used': gpus[0].memoryUsed if gpus else None,
        'gpu_memory_total': gpus[0].memoryTotal if gpus else None,
    }

def send_alert_if_needed(health: dict):
    """ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼"""
    if health['gpu_temp'] > 80:
        logging.critical(f"GPU ê³¼ì—´: {health['gpu_temp']}Â°C")
    
    if health['disk_percent'] > 90:
        logging.critical(f"ë””ìŠ¤í¬ ë¶€ì¡±: {health['disk_percent']}%")
```

### 9.2 í”„ë¡œë©”í…Œìš°ìŠ¤ ë©”íŠ¸ë¦­ (ì„ íƒì‚¬í•­)

```python
# utils/metrics.py
from prometheus_client import Counter, Gauge

posts_crawled = Counter('posts_crawled_total', 'Total posts crawled')
posts_processed = Counter('posts_processed_total', 'Total posts processed')
gpu_memory_usage = Gauge('gpu_memory_usage_bytes', 'GPU memory usage')
processing_time = Gauge('processing_time_seconds', 'Time to process one post')
```

---

## 10. ë°°í¬ ë° ìš´ì˜

### 10.1 ì´ˆê¸° ì„¤ì •

```bash
# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ í¸ì§‘ (DB ë¹„ë°€ë²ˆí˜¸ ë“±)

# 2. Docker ë¹Œë“œ ë° ì‹¤í–‰
docker-compose build
docker-compose up -d

# 3. DB ì´ˆê¸°í™” í™•ì¸
docker-compose logs db | grep "ready for connections"

# 4. ëŒ€ì‹œë³´ë“œ ì ‘ì†
open http://localhost:8501
```

### 10.2 ë°±ì—… ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# scripts/backup.sh

BACKUP_DIR="/mnt/backup/wagglebot"
DATE=$(date +%Y%m%d_%H%M%S)

# DB ë°±ì—…
docker exec wagglebot_db mysqldump -u root -p${DB_ROOT_PASSWORD} wagglebot \
  > ${BACKUP_DIR}/db_${DATE}.sql

# ë¯¸ë””ì–´ íŒŒì¼ ë°±ì—…
rsync -av --progress ./media/ ${BACKUP_DIR}/media_${DATE}/

# 7ì¼ ì´ìƒ ëœ ë°±ì—… ì‚­ì œ
find ${BACKUP_DIR} -type f -mtime +7 -delete

echo "Backup completed: ${DATE}"
```

---

## 11. í™•ì¥ ë¡œë“œë§µ

### Phase 1 (ì™„ë£Œ)
- âœ… í¬ë¡¤ëŸ¬ ì¸í”„ë¼ (BaseCrawler)
- âœ… DB ìŠ¤í‚¤ë§ˆ
- âœ… Streamlit ëŒ€ì‹œë³´ë“œ

### Phase 2 (ì§„í–‰ ì¤‘)
- ğŸš§ AI ì›Œì»¤ (LLM/TTS)
- ğŸš§ ì˜ìƒ ë Œë”ë§
- ğŸš§ VRAM ê´€ë¦¬

### Phase 3 (ê³„íš)
- ğŸ“‹ YouTube ì—…ë¡œë”
- ğŸ“‹ ë©€í‹° í”Œë«í¼ (TikTok, Instagram)
- ğŸ“‹ ê³ ê¸‰ ì˜ìƒ íš¨ê³¼ (Ken Burns, ì „í™˜)
- ğŸ“‹ ë¶„ì„ ëŒ€ì‹œë³´ë“œ

---

## 12. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 12.1 OOM (Out of Memory)
**ì¦ìƒ:** CUDA out of memory ì—ëŸ¬  
**í•´ê²°:** `gpu_manager.unload_all()` í˜¸ì¶œ, ì–‘ìí™” í™•ì¸

### 12.2 FFmpeg ì¸ì½”ë”© ì‹¤íŒ¨
**ì¦ìƒ:** h264_nvenc ì½”ë± ì‚¬ìš© ë¶ˆê°€  
**í•´ê²°:** `nvidia-smi` í™•ì¸, Docker GPU ë§¤í•‘ ì¬ì‹œì‘

### 12.3 DB ì»¤ë„¥ì…˜ í’€ ê³ ê°ˆ
**ì¦ìƒ:** Too many connections  
**í•´ê²°:** `with SessionLocal()` íŒ¨í„´ ì¤€ìˆ˜ í™•ì¸

---

## ë¶€ë¡: ì°¸ì¡° ìë£Œ

- **SQLAlchemy ORM:** https://docs.sqlalchemy.org/
- **FFmpeg NVENC:** https://docs.nvidia.com/video-technologies/
- **Transformers ì–‘ìí™”:** https://huggingface.co/docs/transformers/quantization
- **MoviePy:** https://zulko.github.io/moviepy/
