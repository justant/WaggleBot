"""DCInside ì´ë¯¸ì§€ 5ì¥ ì´ìƒ í¬ë¡¤ë§ â†’ ëŒ€ì‹œë³´ë“œ ì´ë¯¸ì§€ ë¡œë”© E2E í…ŒìŠ¤íŠ¸.

ì‚¬ìš©ë²•:
    python -m test.test_dc_images
"""

import logging
import re
import sys
import time
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup

# â”€â”€ ë¡œê¹… â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger(__name__)

# â”€â”€ ìƒìˆ˜ â”€â”€
MIN_IMAGES = 5
MAX_POSTS_TO_SCAN = 30  # ìµœëŒ€ ìŠ¤ìº”í•  ê²Œì‹œê¸€ ìˆ˜
TARGET_SECTIONS = [
    "https://gall.dcinside.com/board/lists/?id=dcbest",
    "https://gall.dcinside.com/board/lists/?id=hit",
]
_DC_PLACEHOLDERS = (
    "gallview_loading_ori.gif", "trans.gif", "img.gif",
    "loading_image.gif", "blank.gif",
)
_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/131.0.0.0 Safari/537.36"
)


# =====================================================================
# Phase 1: í¬ë¡¤ëŸ¬ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ URL ìˆ˜ì§‘
# =====================================================================

def create_crawl_session() -> requests.Session:
    """í¬ë¡¤ëŸ¬(BaseCrawler)ì™€ ë™ì¼í•œ ì„¸ì…˜ ìƒì„±."""
    sess = requests.Session()
    sess.headers.update({
        "User-Agent": _UA,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://www.dcinside.com/",
    })
    return sess


def fetch_listing(sess: requests.Session) -> list[dict]:
    """DCInside ì‹¤ë² /í›ê°¤ ëª©ë¡ì—ì„œ ê²Œì‹œê¸€ URL ìˆ˜ì§‘."""
    posts: list[dict] = []
    seen: set[str] = set()

    for section_url in TARGET_SECTIONS:
        try:
            resp = sess.get(section_url, timeout=15)
            resp.raise_for_status()
        except Exception as e:
            log.warning("ëª©ë¡ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: %s â€” %s", section_url, e)
            continue

        soup = BeautifulSoup(resp.text, "html.parser")

        # í…Œì´ë¸” ê¸°ë°˜ ë ˆì´ì•„ì›ƒ
        rows = soup.select("table.gall_list tbody tr.us-post")
        if not rows:
            rows = soup.select("tr.ub-content")

        for row in rows:
            if "notice" in " ".join(row.get("class", [])):
                continue
            link = (
                row.select_one("td.gall_tit a:first-child")
                or row.select_one("a.newtxt")
                or row.select_one("a[href*='/board/view/']")
            )
            if not link:
                continue

            href = link.get("href", "")
            if href in seen:
                continue
            seen.add(href)

            url = ("https://gall.dcinside.com" + href) if href.startswith("/") else href
            title = link.get_text(strip=True)[:60]
            posts.append({"url": url, "title": title})

        log.info("  ì„¹ì…˜ %s â†’ %dê°œ ê²Œì‹œê¸€", section_url.split("id=")[1], len(posts))
        time.sleep(0.5)

    return posts


def parse_images_from_post(sess: requests.Session, url: str) -> dict:
    """ê²Œì‹œê¸€ HTMLì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ (í¬ë¡¤ëŸ¬ ë¡œì§ ì¬í˜„).

    Returns:
        {
            "title": str,
            "img_tags_total": int,       # body ë‚´ <img> ì´ ê°œìˆ˜
            "img_tags_lazy": int,        # class="lazy" ì¸ íƒœê·¸ ìˆ˜
            "images_normal": list[str],  # data-original/src ë°©ì‹ ì¶”ì¶œ ê²°ê³¼
            "images_regex": list[str],   # ì •ê·œì‹ fallback ì¶”ê°€ë¶„
            "images_all": list[str],     # ìµœì¢… í•©ì‚° ê²°ê³¼
            "raw_attrs": list[dict],     # ë””ë²„ê¹…ìš©: ê° <img>ì˜ ì£¼ìš” ì†ì„±
        }
    """
    resp = sess.get(url, timeout=15)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    title_el = (
        soup.select_one("span.title_subject")
        or soup.select_one("h4.title span")
        or soup.select_one("h3.title")
    )
    title = title_el.get_text(strip=True) if title_el else "(ì œëª© ì—†ìŒ)"

    body_el = soup.select_one("div.writing_view_box")
    if not body_el:
        return {
            "title": title, "img_tags_total": 0, "img_tags_lazy": 0,
            "images_normal": [], "images_regex": [], "images_all": [],
            "raw_attrs": [],
        }

    # â”€â”€ 1) ê¸°ë³¸ ì¶”ì¶œ (í¬ë¡¤ëŸ¬ ë¡œì§) â”€â”€
    all_imgs = body_el.select("img:not(.og-img)")
    lazy_count = sum(1 for img in all_imgs if "lazy" in (img.get("class") or []))

    raw_attrs: list[dict] = []
    images_normal: list[str] = []
    for img in all_imgs:
        attrs = {
            "src": img.get("src", "")[:100],
            "data-original": img.get("data-original", "")[:100],
            "data-lazy": img.get("data-lazy", "")[:100],
            "data-src": img.get("data-src", "")[:100],
            "class": img.get("class", []),
        }
        raw_attrs.append(attrs)

        src = (
            img.get("data-original")
            or img.get("data-lazy")
            or img.get("data-src")
            or img.get("data-lazy-src")
            or img.get("src")
            or ""
        )
        if src.startswith("//"):
            src = "https:" + src
        if src.startswith("http") and not any(ph in src for ph in _DC_PLACEHOLDERS):
            images_normal.append(src)

    # â”€â”€ 2) ì •ê·œì‹ fallback â”€â”€
    seen = set(images_normal)
    body_html = str(body_el)
    images_regex: list[str] = []
    for raw in re.findall(
        r'(?:https?:)?//(?:dcimg\d*|image)\.dcinside\.com/[^\s"\'<>]+',
        body_html,
    ):
        url_clean = "https:" + raw if raw.startswith("//") else raw
        if (
            url_clean not in seen
            and not any(ph in url_clean for ph in _DC_PLACEHOLDERS)
            and ("viewimage.php" in url_clean or re.search(r'\.(?:jpg|jpeg|png|gif|webp)', url_clean, re.IGNORECASE))
        ):
            images_regex.append(url_clean)
            seen.add(url_clean)

    images_all = images_normal + images_regex

    return {
        "title": title,
        "img_tags_total": len(all_imgs),
        "img_tags_lazy": lazy_count,
        "images_normal": images_normal,
        "images_regex": images_regex,
        "images_all": images_all,
        "raw_attrs": raw_attrs,
    }


# =====================================================================
# Phase 2: ëŒ€ì‹œë³´ë“œ ì´ë¯¸ì§€ ìŠ¬ë¼ì´ë”ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
# =====================================================================

def create_dashboard_session() -> requests.Session:
    """image_slider.pyì˜ _get_dc_session() ì¬í˜„."""
    sess = requests.Session()
    sess.headers.update({
        "User-Agent": _UA,
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    })
    # ì¿ í‚¤ ì›Œë°ì—…
    try:
        sess.get("https://www.dcinside.com/", timeout=10)
        log.info("  ëŒ€ì‹œë³´ë“œ ì„¸ì…˜ ì¿ í‚¤ ì›Œë°ì—… OK (cookies=%d)", len(sess.cookies))
    except Exception:
        log.warning("  ëŒ€ì‹œë³´ë“œ ì„¸ì…˜ ì¿ í‚¤ ì›Œë°ì—… ì‹¤íŒ¨")
    return sess


def fetch_image_like_dashboard(sess: requests.Session, url: str) -> dict:
    """image_slider.pyì˜ _fetch_image() ë¡œì§ ì¬í˜„.

    Returns:
        {"ok": bool, "status": int, "size": int, "content_type": str, "error": str}
    """
    hostname = urlparse(url).hostname or ""
    is_dc = any(hostname.endswith(d) for d in ("dcinside.com", "dcinside.co.kr"))

    try:
        if is_dc:
            resp = sess.get(
                url,
                timeout=(5, 15),
                headers={
                    "Referer": "https://gall.dcinside.com/",
                    "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
                    "Sec-Fetch-Dest": "image",
                    "Sec-Fetch-Mode": "no-cors",
                    "Sec-Fetch-Site": "cross-site",
                },
            )
        else:
            resp = requests.get(
                url,
                timeout=(5, 10),
                headers={
                    "Referer": f"{urlparse(url).scheme}://{urlparse(url).netloc}/",
                    "User-Agent": _UA,
                    "Accept": "image/*,*/*;q=0.8",
                },
            )
        resp.raise_for_status()
        data = resp.content
        ct = resp.headers.get("Content-Type", "")
        size = len(data)

        if size < 200:
            return {"ok": False, "status": resp.status_code, "size": size, "content_type": ct, "error": f"í”Œë ˆì´ìŠ¤í™€ë” ì˜ì‹¬ ({size}B)"}

        return {"ok": True, "status": resp.status_code, "size": size, "content_type": ct, "error": ""}

    except requests.HTTPError as e:
        code = e.response.status_code if e.response is not None else 0
        return {"ok": False, "status": code, "size": 0, "content_type": "", "error": str(e)}
    except Exception as e:
        return {"ok": False, "status": 0, "size": 0, "content_type": "", "error": str(e)}


# =====================================================================
# Main
# =====================================================================

def main() -> None:
    log.info("=" * 60)
    log.info("DCInside ì´ë¯¸ì§€ E2E í…ŒìŠ¤íŠ¸ ì‹œì‘")
    log.info("  ì¡°ê±´: ì´ë¯¸ì§€ %dì¥ ì´ìƒì¸ ê²Œì‹œê¸€ ì°¾ê¸°", MIN_IMAGES)
    log.info("=" * 60)

    # â”€â”€ Step 1: ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ â”€â”€
    crawl_sess = create_crawl_session()
    log.info("\n[Step 1] ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘...")
    posts = fetch_listing(crawl_sess)
    if not posts:
        log.error("ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    log.info("  ì´ %dê°œ ê²Œì‹œê¸€ ë°œê²¬", len(posts))

    # â”€â”€ Step 2: ì´ë¯¸ì§€ 5ì¥ ì´ìƒ ê²Œì‹œê¸€ ì°¾ê¸° â”€â”€
    log.info("\n[Step 2] ì´ë¯¸ì§€ %dì¥ ì´ìƒ ê²Œì‹œê¸€ ê²€ìƒ‰ (ìµœëŒ€ %dê°œ ìŠ¤ìº”)...", MIN_IMAGES, MAX_POSTS_TO_SCAN)
    target_post = None

    for i, post in enumerate(posts[:MAX_POSTS_TO_SCAN]):
        try:
            result = parse_images_from_post(crawl_sess, post["url"])
        except Exception as e:
            log.warning("  [%d] íŒŒì‹± ì‹¤íŒ¨: %s â€” %s", i + 1, post["title"][:30], e)
            time.sleep(0.5)
            continue

        img_count = len(result["images_all"])
        log.info(
            "  [%d/%d] %s â€” <img> %dê°œ (lazy=%d) â†’ ìœ íš¨ ì´ë¯¸ì§€ %dê°œ (ê¸°ë³¸=%d, regex=%d)",
            i + 1, min(len(posts), MAX_POSTS_TO_SCAN),
            result["title"][:30],
            result["img_tags_total"], result["img_tags_lazy"],
            img_count, len(result["images_normal"]), len(result["images_regex"]),
        )

        if img_count >= MIN_IMAGES:
            target_post = {**post, **result}
            log.info("  âœ… ëŒ€ìƒ ê²Œì‹œê¸€ ë°œê²¬!")
            break

        time.sleep(0.5)

    if target_post is None:
        log.error("ì´ë¯¸ì§€ %dì¥ ì´ìƒì¸ ê²Œì‹œê¸€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", MIN_IMAGES)
        sys.exit(1)

    # â”€â”€ Step 3: í¬ë¡¤ëŸ¬ ê²°ê³¼ ìƒì„¸ ì¶œë ¥ â”€â”€
    images = target_post["images_all"]
    log.info("\n[Step 3] í¬ë¡¤ëŸ¬ ì´ë¯¸ì§€ ìˆ˜ì§‘ ê²°ê³¼ ìƒì„¸")
    log.info("  ì œëª©: %s", target_post["title"])
    log.info("  URL:  %s", target_post["url"])
    log.info("  <img> íƒœê·¸: %dê°œ (lazy: %dê°œ)", target_post["img_tags_total"], target_post["img_tags_lazy"])
    log.info("  ê¸°ë³¸ ì¶”ì¶œ: %dì¥ / ì •ê·œì‹ ì¶”ê°€: %dì¥ / í•©ê³„: %dì¥",
             len(target_post["images_normal"]), len(target_post["images_regex"]), len(images))

    log.info("\n  [<img> íƒœê·¸ ì†ì„± ë””ë²„ê¹…]")
    for j, attrs in enumerate(target_post["raw_attrs"]):
        log.info("    img[%d] class=%s", j, attrs["class"])
        log.info("           src           = %s", attrs["src"] or "(ì—†ìŒ)")
        log.info("           data-original = %s", attrs["data-original"] or "(ì—†ìŒ)")
        log.info("           data-lazy     = %s", attrs["data-lazy"] or "(ì—†ìŒ)")
        log.info("           data-src      = %s", attrs["data-src"] or "(ì—†ìŒ)")

    log.info("\n  [ìµœì¢… ì´ë¯¸ì§€ URL ëª©ë¡]")
    for j, url in enumerate(images):
        log.info("    [%d] %s", j + 1, url[:120])

    # â”€â”€ Step 4: ëŒ€ì‹œë³´ë“œ ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸ â”€â”€
    log.info("\n[Step 4] ëŒ€ì‹œë³´ë“œ(image_slider) ë°©ì‹ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸")
    dash_sess = create_dashboard_session()

    results: list[dict] = []
    all_ok = True
    for j, url in enumerate(images):
        r = fetch_image_like_dashboard(dash_sess, url)
        results.append(r)
        status_icon = "âœ…" if r["ok"] else "âŒ"
        log.info(
            "  %s [%d/%d] HTTP %s | %s | %s%s",
            status_icon, j + 1, len(images),
            r["status"] or "ERR",
            f"{r['size']:,}B" if r["size"] else "0B",
            r["content_type"][:30],
            f" â€” {r['error']}" if r["error"] else "",
        )
        if not r["ok"]:
            all_ok = False
        time.sleep(0.3)

    # â”€â”€ ê²°ê³¼ ìš”ì•½ â”€â”€
    ok_count = sum(1 for r in results if r["ok"])
    fail_count = len(results) - ok_count

    log.info("\n" + "=" * 60)
    log.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    log.info("=" * 60)
    log.info("  ê²Œì‹œê¸€:     %s", target_post["title"])
    log.info("  ì´ ì´ë¯¸ì§€:  %dì¥", len(images))
    log.info("  ì„±ê³µ:       %dì¥ âœ…", ok_count)
    log.info("  ì‹¤íŒ¨:       %dì¥ âŒ", fail_count)

    if fail_count > 0:
        log.info("\n  [ì‹¤íŒ¨ ìƒì„¸]")
        for j, (url, r) in enumerate(zip(images, results)):
            if not r["ok"]:
                log.info("    [%d] %s", j + 1, url[:100])
                log.info("         â†’ %s", r["error"])

    if all_ok:
        log.info("\nğŸ‰ ëª¨ë“  ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ! í¬ë¡¤ëŸ¬ + ëŒ€ì‹œë³´ë“œ ì´ë¯¸ì§€ íŒŒì´í”„ë¼ì¸ ì •ìƒ.")
    else:
        log.info("\nâš ï¸  ì¼ë¶€ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨. ìœ„ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)


if __name__ == "__main__":
    main()
