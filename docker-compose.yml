# GPU 환경: RTX 3090 24GB (Quality-First 2막 구조)
services:
  db:
    image: mariadb:11
    restart: always
    environment:
      MARIADB_ROOT_PASSWORD: ${MARIADB_ROOT_PASSWORD:-wagglebot_root}
      MARIADB_DATABASE: wagglebot
      MARIADB_USER: wagglebot
      MARIADB_PASSWORD: ${MARIADB_PASSWORD:-wagglebot}
    ports:
      - "3306:3306"
    volumes:
      - mariadb_data:/var/lib/mysql
    healthcheck:
      test: ["CMD-SHELL", "mariadb-admin ping -h localhost -u root -p${MARIADB_ROOT_PASSWORD} || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  crawler:
    build: .
    restart: unless-stopped
    volumes:
      - ./:/app
      - /app/venv
      - /app/__pycache__
    depends_on:
      db:
        condition: service_healthy
    environment:
      DATABASE_URL: mysql+pymysql://wagglebot:${MARIADB_PASSWORD:-wagglebot}@db/wagglebot
      CRAWL_INTERVAL_HOURS: ${CRAWL_INTERVAL_HOURS:-1}
    command: python main.py

  dashboard:
    build: .
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy
    ports:
      - "${STREAMLIT_PORT:-8501}:8501"
    volumes:
      - ./:/app
      - ./media:/app/media
      - /app/venv
      - /app/__pycache__
    environment:
      DATABASE_URL: mysql+pymysql://wagglebot:${MARIADB_PASSWORD:-wagglebot}@db/wagglebot
      OLLAMA_HOST: "http://host.docker.internal:11434"
      OLLAMA_MODEL: "${OLLAMA_MODEL:-qwen2.5:7b}"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: streamlit run dashboard/app.py --server.port 8501 --server.headless true --server.address 0.0.0.0

  fish-speech:
    image: fishaudio/fish-speech:v1.5.1
    container_name: fish-speech
    restart: unless-stopped
    runtime: nvidia
    ports:
      - "8080:8080"
    volumes:
      - ./assets/voices:/opt/fish-speech/references
      - ./checkpoints:/opt/fish-speech/checkpoints
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    working_dir: /opt/fish-speech
    # v1.5.1 이미지: /opt/fish-speech/ 설치, common.sh 없음, python 직접 실행
    entrypoint:
      - /bin/bash
      - -c
      - >-
        python tools/api_server.py
        --listen 0.0.0.0:8080
        --llama-checkpoint-path checkpoints/fish-speech-1.5
        --decoder-checkpoint-path checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth
        --decoder-config-name firefly_gan_vq
        --half
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/').read()"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s

  comfyui:
    build:
      context: .
      dockerfile: Dockerfile.comfyui
    container_name: comfyui
    restart: unless-stopped
    runtime: nvidia
    ports:
      - "8188:8188"
    volumes:
      - ./checkpoints/ltx-2:/comfyui/models/checkpoints
      - ./checkpoints/text_encoders:/comfyui/models/text_encoders
      - ./checkpoints/latent_upscale_models:/comfyui/models/latent_upscale_models
      - ./checkpoints/loras:/comfyui/models/loras
      - ./comfyui_workflows:/comfyui/custom_workflows
      - ./media/tmp/videos:/comfyui/output
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CLI_ARGS=--listen 0.0.0.0 --lowvram
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8188/system_stats').read()"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  ai_worker:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    restart: unless-stopped
    volumes:
      - ./:/app
      - ./media:/app/media
      - /app/venv
      - /app/__pycache__
    environment:
      OLLAMA_MODEL: "${OLLAMA_MODEL:-qwen2.5:7b}"
      LLM_MODEL: "${OLLAMA_MODEL:-qwen2.5:7b}"
      DEFAULT_MODEL: "${OLLAMA_MODEL:-qwen2.5:7b}"
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: all
      DATABASE_URL: mysql+pymysql://wagglebot:${MARIADB_PASSWORD:-wagglebot}@db/wagglebot
      OLLAMA_HOST: "http://host.docker.internal:11434"
      OLLAMA_BASE_URL: "http://host.docker.internal:11434"
      MEDIA_DIR: /app/media
      FISH_SPEECH_URL: "http://fish-speech:8080"
      COMFYUI_URL: "http://comfyui:8188"
      VIDEO_GEN_ENABLED: "${VIDEO_GEN_ENABLED:-false}"
      VIDEO_OUTPUT_DIR: "/app/media/tmp/videos"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      db:
        condition: service_healthy
      fish-speech:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: python -m ai_worker.main

  monitoring:
    build: .
    restart: unless-stopped
    volumes:
      - ./:/app
      - /app/venv
      - /app/__pycache__
    depends_on:
      db:
        condition: service_healthy
    environment:
      DATABASE_URL: mysql+pymysql://wagglebot:${MARIADB_PASSWORD:-wagglebot}@db/wagglebot
      MONITORING_ENABLED: ${MONITORING_ENABLED:-true}
      HEALTH_CHECK_INTERVAL: ${HEALTH_CHECK_INTERVAL:-300}
      GPU_TEMP_WARNING: ${GPU_TEMP_WARNING:-75}
      GPU_TEMP_CRITICAL: ${GPU_TEMP_CRITICAL:-80}
      DISK_USAGE_WARNING: ${DISK_USAGE_WARNING:-80}
      DISK_USAGE_CRITICAL: ${DISK_USAGE_CRITICAL:-90}
      MEMORY_USAGE_WARNING: ${MEMORY_USAGE_WARNING:-85}
      MEMORY_USAGE_CRITICAL: ${MEMORY_USAGE_CRITICAL:-95}
      EMAIL_ALERTS_ENABLED: ${EMAIL_ALERTS_ENABLED:-false}
      SMTP_HOST: ${SMTP_HOST:-smtp.gmail.com}
      SMTP_PORT: ${SMTP_PORT:-587}
      SMTP_USER: ${SMTP_USER:-}
      SMTP_PASSWORD: ${SMTP_PASSWORD:-}
      ALERT_EMAIL_TO: ${ALERT_EMAIL_TO:-}
      SLACK_ALERTS_ENABLED: ${SLACK_ALERTS_ENABLED:-false}
      SLACK_WEBHOOK_URL: ${SLACK_WEBHOOK_URL:-}
    command: python -m monitoring.daemon

volumes:
  mariadb_data:
